{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Model Pruning PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPph1m19m9EfExrPFM+HGwY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujdutt9/PyTorch-DeepLearning/blob/master/ML_Model_Pruning_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gERb2WFL4lpy",
        "colab_type": "text"
      },
      "source": [
        "# ML Model Pruning using PyTorch\n",
        "\n",
        "Ref.: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#extending-torch-nn-utils-prune-with-custom-pruning-functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMaj-r4YRXC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Dependencies\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOl9RUnGWLab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fa74072-196d-4716-c97b-df14394acec8"
      },
      "source": [
        "# Check PyTorch Version\n",
        "torch.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9i8OSeYWO3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a simple LeNET Model Architecture\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAml5AQ9WZ4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set device to train the model on\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydn8sLzHWe5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3a66832e-e450-4d2d-af90-e075c7a0c523"
      },
      "source": [
        "# Model Initialization\n",
        "model = LeNet().to(device=device)\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gl4tBTwWvS6",
        "colab_type": "text"
      },
      "source": [
        "#Inspect a Module in the Defined Model\n",
        "\n",
        "Here, we are loading a single layer of the defined ML model, calling the Conv1 layer as a module, with it's initial **Un-pruned weights**.\n",
        "\n",
        "Then, we are just printing out the un-pruned weights and bias values for the Conv1 layer.\n",
        "\n",
        "**NOTE:** See how the name **'weight'** and **'bias'** appears for the values in the Conv1 layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3UavR4dWu_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "4ad687fd-f9e8-4e24-a344-ddbe26fe4278"
      },
      "source": [
        "# Take first layer and print out the Un-Pruned parameters i.e.\n",
        "# weights & biases\n",
        "module = model.conv1\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight', Parameter containing:\n",
            "tensor([[[[-0.2416,  0.1972,  0.0812],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.2302]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.1141, -0.0150, -0.1813],\n",
            "          [ 0.0240,  0.1568,  0.1943],\n",
            "          [ 0.1124,  0.0140, -0.0316]]],\n",
            "\n",
            "\n",
            "        [[[-0.0146,  0.1459, -0.1749],\n",
            "          [-0.1287,  0.1655, -0.1921],\n",
            "          [ 0.0227, -0.1735,  0.2135]]],\n",
            "\n",
            "\n",
            "        [[[-0.1326,  0.0777, -0.1734],\n",
            "          [ 0.2746, -0.0787,  0.1938],\n",
            "          [ 0.2148,  0.1972,  0.0917]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.1422, -0.1531],\n",
            "          [-0.2701,  0.0163,  0.1772],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], requires_grad=True)), ('bias', Parameter containing:\n",
            "tensor([ 0.1209,  0.3181,  0.0193, -0.2359,  0.2840, -0.2947],\n",
            "       requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqbSZQ4IWlBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6a1d299-92cf-4f78-cf0f-bf078f8348ca"
      },
      "source": [
        "# This module has no named buffers for now\n",
        "print(list(module.named_buffers()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CU-6TTggLwf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "30f99b7f-4855-422a-f280-d0919310b449"
      },
      "source": [
        "# Take first layer and print out the Un-pruned parameters i.e.\n",
        "# weights & biases\n",
        "module_fc = model.fc1\n",
        "print(list(module_fc.named_parameters()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight', Parameter containing:\n",
            "tensor([[-0.0435,  0.0392, -0.0189,  ...,  0.0163, -0.0469,  0.0444],\n",
            "        [-0.0127,  0.0344,  0.0455,  ..., -0.0046, -0.0064,  0.0457],\n",
            "        [ 0.0081,  0.0108,  0.0210,  ..., -0.0358,  0.0033, -0.0339],\n",
            "        ...,\n",
            "        [ 0.0372, -0.0162, -0.0239,  ...,  0.0357,  0.0484,  0.0002],\n",
            "        [-0.0243,  0.0014, -0.0417,  ..., -0.0239,  0.0293,  0.0421],\n",
            "        [-0.0185, -0.0418, -0.0248,  ..., -0.0375, -0.0463,  0.0091]],\n",
            "       requires_grad=True)), ('bias', Parameter containing:\n",
            "tensor([-0.0302, -0.0452,  0.0378, -0.0282, -0.0371, -0.0245, -0.0288, -0.0206,\n",
            "         0.0065, -0.0115, -0.0043, -0.0088, -0.0496, -0.0284,  0.0346,  0.0362,\n",
            "         0.0363,  0.0235,  0.0384, -0.0388,  0.0079, -0.0074,  0.0469, -0.0407,\n",
            "         0.0460, -0.0468,  0.0153,  0.0010,  0.0115, -0.0128,  0.0452, -0.0266,\n",
            "        -0.0278,  0.0468,  0.0331,  0.0175, -0.0459, -0.0147, -0.0239,  0.0475,\n",
            "         0.0215, -0.0402,  0.0439, -0.0340, -0.0342,  0.0041, -0.0137, -0.0367,\n",
            "         0.0459,  0.0049,  0.0230,  0.0426,  0.0272,  0.0480,  0.0343,  0.0104,\n",
            "         0.0105,  0.0174,  0.0387,  0.0228,  0.0137, -0.0214,  0.0025, -0.0292,\n",
            "         0.0412, -0.0008,  0.0266, -0.0374,  0.0449,  0.0417, -0.0401,  0.0083,\n",
            "         0.0488, -0.0330, -0.0489,  0.0395, -0.0471, -0.0472, -0.0320, -0.0110,\n",
            "         0.0228, -0.0230,  0.0071, -0.0213, -0.0444, -0.0037,  0.0063,  0.0438,\n",
            "         0.0219, -0.0025, -0.0066,  0.0040,  0.0242,  0.0457, -0.0086,  0.0218,\n",
            "        -0.0349, -0.0454,  0.0335, -0.0136, -0.0200, -0.0215, -0.0065, -0.0259,\n",
            "        -0.0267, -0.0321,  0.0011, -0.0319,  0.0043, -0.0445,  0.0436, -0.0195,\n",
            "         0.0320, -0.0323, -0.0087,  0.0293,  0.0408, -0.0401, -0.0234, -0.0266],\n",
            "       requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPgiGAMNXGxN",
        "colab_type": "text"
      },
      "source": [
        "# Pruning the Module\n",
        "\n",
        "In this, we'll prune the first layer i.e. the Conv1 module of the model.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. Firstly, we'll select a pruning technique among those available in **torch.nn.utils.prune**.\n",
        "\n",
        "2. Then, **specify the module** i.e. which layer in the model you want to prune and the **name of the parameter to prune**, i.e. **un-pruned layer weight or bias**, within that module. \n",
        "\n",
        "3. Finally, using the adequate keyword arguments required by the selected pruning technique, specify the pruning parameters.\n",
        "\n",
        "\n",
        "In this example, we will **prune at random 30% of the connections** in the **parameter named weight in the conv1 layer**.\n",
        "\n",
        "The pruning technique function takes the following arguments:\n",
        "\n",
        "1. **module:** is passed as the first argument to the function.\n",
        "\n",
        "2. **name:** identifies the parameter within that module using its string identifier i.e 'weight' or 'bias'.\n",
        "\n",
        "3. **amount:** indicates either the percentage of connections to prune (if it is a float between 0. and 1.), or the absolute number of connections to prune (if it is a non-negative integer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TouR7r7hXFLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db6eea59-5bc0-4d28-ca9f-40f2553101d8"
      },
      "source": [
        "# Prune at random 30% of the weights in Conv1 layer\n",
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qapMl2SY6vj",
        "colab_type": "text"
      },
      "source": [
        "## Renaming the Original Layer Parameters\n",
        "\n",
        "Pruning acts by removing weight from the parameters and replacing it with a new parameter called **weight_orig (i.e. appending \"_orig\" to the initial parameter name)**. \n",
        "\n",
        "**weight_orig** stores the **un-pruned version of the tensor (weight or bias)**. The bias was not pruned, so it will remain intact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NBS54TfYyzu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "3cdb3e12-7e39-4e61-a028-024e428db869"
      },
      "source": [
        "# See how after applying Pruning, the Original Weights of the Model are\n",
        "# stored and renamed as 'weight_orig'.\n",
        "# The 'weight_orig' are un-pruned weights of the layer.\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([ 0.1209,  0.3181,  0.0193, -0.2359,  0.2840, -0.2947],\n",
            "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[[[-0.2416,  0.1972,  0.0812],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.2302]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.1141, -0.0150, -0.1813],\n",
            "          [ 0.0240,  0.1568,  0.1943],\n",
            "          [ 0.1124,  0.0140, -0.0316]]],\n",
            "\n",
            "\n",
            "        [[[-0.0146,  0.1459, -0.1749],\n",
            "          [-0.1287,  0.1655, -0.1921],\n",
            "          [ 0.0227, -0.1735,  0.2135]]],\n",
            "\n",
            "\n",
            "        [[[-0.1326,  0.0777, -0.1734],\n",
            "          [ 0.2746, -0.0787,  0.1938],\n",
            "          [ 0.2148,  0.1972,  0.0917]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.1422, -0.1531],\n",
            "          [-0.2701,  0.0163,  0.1772],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrFfn9rhZNFE",
        "colab_type": "text"
      },
      "source": [
        "## Weight Mask\n",
        "\n",
        "When we call prune method on a layer/module, it renames the original weights and creates a mask, with the shape same as the original un-pruned weights, in the module buffer.\n",
        "\n",
        "The pruning mask generated by the pruning technique selected above is saved as a module buffer named **weight_mask** (i.e. appending **\"_mask\"** to the initial parameter name).\n",
        "\n",
        "This **'weight_mask'** is later on **used to zero out the corresponding values of the original weight tensor ('weight_orig')**. So, what remains is the values in the 'weight_orig' corresponding to the \"1\" in the 'weight_mask'. Hence, the weights are pruned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io1mIqXYZDNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "1bd1f5ed-703e-4426-f6f7-0c8c2fe6e636"
      },
      "source": [
        "# The weight_mask tells which weight values to prune and which to leave.\n",
        "# All weights in \"weight_orig\" corresponding to a \"1\" in the mask remain intact, \n",
        "# rest all weights corresponding to \"0\" in mask are pruned.\n",
        "print(list(module.named_buffers()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_mask', tensor([[[[0., 1., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1.],\n",
            "          [1., 1., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [0., 0., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 0., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 1.]]]]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5moADi7Z8eM",
        "colab_type": "text"
      },
      "source": [
        "**For the forward pass to work without modification, the weight attribute needs to exist.** The pruning techniques implemented in torch.nn.utils.prune **compute the pruned version of the weight (by combining the mask with the original parameter) and store them in the attribute weight.** Note, this is no longer a parameter of the module, it is now simply an attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2HCMwxEZcmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "2636fc3d-e2ae-4b23-c0ee-3546790424a0"
      },
      "source": [
        "# Pruned Weights for Conv1 Layer\n",
        "# Weights set to \"0\" for places where mask has a \"0\"\n",
        "print(module.weight)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.0000,  0.1972,  0.0000],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.1813],\n",
            "          [ 0.0240,  0.1568,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0146,  0.1459, -0.1749],\n",
            "          [-0.0000,  0.0000, -0.1921],\n",
            "          [ 0.0227, -0.1735,  0.2135]]],\n",
            "\n",
            "\n",
            "        [[[-0.1326,  0.0777, -0.1734],\n",
            "          [ 0.2746, -0.0000,  0.1938],\n",
            "          [ 0.0000,  0.1972,  0.0917]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.0000, -0.1531],\n",
            "          [-0.0000,  0.0163,  0.0000],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xE2CQXZOiAK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "68cb1094-1ac3-4c9f-fba1-64597f9703cd"
      },
      "source": [
        "# This verifies that the original parameters are not changed unl you do so\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([ 0.1209,  0.3181,  0.0193, -0.2359,  0.2840, -0.2947],\n",
            "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[[[-0.2416,  0.1972,  0.0812],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.2302]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.1141, -0.0150, -0.1813],\n",
            "          [ 0.0240,  0.1568,  0.1943],\n",
            "          [ 0.1124,  0.0140, -0.0316]]],\n",
            "\n",
            "\n",
            "        [[[-0.0146,  0.1459, -0.1749],\n",
            "          [-0.1287,  0.1655, -0.1921],\n",
            "          [ 0.0227, -0.1735,  0.2135]]],\n",
            "\n",
            "\n",
            "        [[[-0.1326,  0.0777, -0.1734],\n",
            "          [ 0.2746, -0.0787,  0.1938],\n",
            "          [ 0.2148,  0.1972,  0.0917]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.1422, -0.1531],\n",
            "          [-0.2701,  0.0163,  0.1772],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93PsZ9yLalpr",
        "colab_type": "text"
      },
      "source": [
        "Finally, **pruning is applied prior to each forward pass using PyTorch’s forward_pre_hooks**. Specifically, **when the module is pruned**, as we have done here, **it will acquire a forward_pre_hook for each parameter associated with it that gets pruned**. In this case, since we have so far only pruned the original parameter named weight, only one hook will be present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A-ahNJFaKea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6d89064-e607-408d-a95b-4d7725c952b2"
      },
      "source": [
        "# Forward Pass pre Hook\n",
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f2ec9937ba8>)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5rzM-2IbCXH",
        "colab_type": "text"
      },
      "source": [
        "For completeness, we can now prune the bias too, to see how the parameters, buffers, hooks, and attributes of the module change. Just for the sake of trying out another pruning technique, here **we prune the 3 smallest entries in the bias by L1 norm**, as implemented in the **l1_unstructured pruning** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgK0hbMfa6ec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79c8cb59-ee6f-4856-d4c5-46a846f8e517"
      },
      "source": [
        "# Bias Pruning\n",
        "prune.l1_unstructured(module, name=\"bias\", amount=3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16hrVm3fbSIi",
        "colab_type": "text"
      },
      "source": [
        "We now expect the named parameters to include both **weight_orig (from before) and bias_orig**. The buffers will include **weight_mask and bias_mask**. The pruned versions of the two tensors will exist as module attributes, and the module will now have two forward_pre_hooks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDMhCDL2bNVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "1cd3dde5-1393-45eb-c62f-cf31fdfb460b"
      },
      "source": [
        "# Module Original Named Parameters\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_orig', Parameter containing:\n",
            "tensor([[[[-0.2416,  0.1972,  0.0812],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.2302]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.1141, -0.0150, -0.1813],\n",
            "          [ 0.0240,  0.1568,  0.1943],\n",
            "          [ 0.1124,  0.0140, -0.0316]]],\n",
            "\n",
            "\n",
            "        [[[-0.0146,  0.1459, -0.1749],\n",
            "          [-0.1287,  0.1655, -0.1921],\n",
            "          [ 0.0227, -0.1735,  0.2135]]],\n",
            "\n",
            "\n",
            "        [[[-0.1326,  0.0777, -0.1734],\n",
            "          [ 0.2746, -0.0787,  0.1938],\n",
            "          [ 0.2148,  0.1972,  0.0917]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.1422, -0.1531],\n",
            "          [-0.2701,  0.0163,  0.1772],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
            "tensor([ 0.1209,  0.3181,  0.0193, -0.2359,  0.2840, -0.2947],\n",
            "       requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziuWkzgGbbsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "efa23e09-4db0-48ca-c79f-1f2dac76feae"
      },
      "source": [
        "# The weight_mask tells which weight values to prune and which to leave.\n",
        "# All weights in \"weight_orig\" corresponding to a \"1\" in the mask remain intact, \n",
        "# rest all weights corresponding to \"0\" in mask are pruned.\n",
        "# Same for bias values in \"bias_orig\"\n",
        "print(list(module.named_buffers()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_mask', tensor([[[[0., 1., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1.],\n",
            "          [1., 1., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [0., 0., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 0., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 1.]]]])), ('bias_mask', tensor([0., 1., 0., 0., 1., 1.]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z4YLyWhb6hI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d36e2697-75b9-4fe9-d731-2755c1c2967d"
      },
      "source": [
        "# Pruned Bias values for Conv1 Layer\n",
        "# Bias set to \"0\" for places where mask has a \"0\"\n",
        "print(module.bias)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.0000,  0.3181,  0.0000, -0.0000,  0.2840, -0.2947],\n",
            "       grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp4dq3YcblFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b5050ee8-37ab-43c5-911f-0b2af4387a64"
      },
      "source": [
        "# Forward Pass pre Hook\n",
        "# There should be 2 hooks, one for weights, other for bias\n",
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f2ec9937ba8>), (1, <torch.nn.utils.prune.L1Unstructured object at 0x7f2ec994f6a0>)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGhiudlVzl5M",
        "colab_type": "text"
      },
      "source": [
        "# Iterative Pruning\n",
        "\n",
        "The same parameter in a module can be pruned multiple times, with the effect of the various pruning calls being equal to the combination of the various masks applied in series. \n",
        "\n",
        "The combination of a new mask with the old mask is handled by the **PruningContainer**’s **compute_mask method**.\n",
        "\n",
        "Say, for example, that we now want to further prune **module.weight**, this time **using structured pruning along the 0th axis of the tensor** (the 0th axis corresponds to the output channels of the convolutional layer and has dimensionality 6 for conv1), based on the channels’ L2 norm. \n",
        "\n",
        "This can be achieved using the **ln_structured** function, with **n=2 and dim=0**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldZGZt9wzwJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9886a297-f71d-4238-ba5b-9469cec94eff"
      },
      "source": [
        "# Iterativively Prune the Model i.e.\n",
        "# Serially apply multiple masks to the Model Layer (weight/bias)\n",
        "\n",
        "# Pruning Parameters\n",
        "# module: Model Layer\n",
        "# name: Model layer parameter to prune\n",
        "# amount: Percentage of pruning to perform for this layer\n",
        "# n: Matrix Norm to use. Ref: https://pytorch.org/docs/master/generated/torch.norm.html#torch.norm\n",
        "# dim: index of dim along which to prune the values in the tensor\n",
        "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BUSiHHu0TpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "7cc463a4-0ddb-42fa-8d45-55d6f8ade399"
      },
      "source": [
        "# As we can verify, this will zero out all the connections corresponding to\n",
        "# 50% (3 out of 6) of the channels, while preserving the action of the\n",
        "# previous mask.\n",
        "print(module.weight)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.0000,  0.1972,  0.0000],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.0000, -0.1531],\n",
            "          [-0.0000,  0.0163,  0.0000],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1IKpnvR0B-c",
        "colab_type": "text"
      },
      "source": [
        "The corresponding hook will now be of type **torch.nn.utils.prune.PruningContainer**, and will store the history of pruning applied to the weight parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWYkFEq_z11m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a38d8efd-02e4-4954-f9fc-5cc3e2a57c6d"
      },
      "source": [
        "for hook in module._forward_pre_hooks.values():\n",
        "  # Select the correct hook\n",
        "  if hook._tensor_name == \"weight\":\n",
        "    break\n",
        "\n",
        "# Puning History in the Container\n",
        "print(list(hook))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<torch.nn.utils.prune.RandomUnstructured object at 0x7f2ec9937ba8>, <torch.nn.utils.prune.LnStructured object at 0x7f2ec994f390>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sHO-1AP0gwl",
        "colab_type": "text"
      },
      "source": [
        "# Serializing Pruned Model\n",
        "\n",
        "All relevant tensors, including the **mask buffers** and the original parameters used to compute the pruned tensors are stored in the model’s **state_dict** and can therefore be easily serialized and saved, if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnmwmZ0b0srg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b0c296e9-8949-40c2-9a73-bdac166c30e3"
      },
      "source": [
        "print(model.state_dict().keys())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4urIwy1U0xe8",
        "colab_type": "text"
      },
      "source": [
        "# Remove pruning re-parametrization\n",
        "\n",
        "To make the pruning permanent, remove the re-parametrization in terms of weight_orig and weight_mask, and remove the forward_pre_hook, we can use the remove functionality from torch.nn.utils.prune. \n",
        "\n",
        "Note that this doesn’t undo the pruning, as if it never happened. It simply makes it permanent, instead, by reassigning the parameter weight to the model parameters, in its pruned version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye4LPlj-3OHg",
        "colab_type": "text"
      },
      "source": [
        "## Prior to removing the re-parametrization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8BohhEm0wfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "05cd4aeb-8ee8-4746-8be5-284f90282173"
      },
      "source": [
        "# Layer Parameters Prior to removing the re-parametrization\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_orig', Parameter containing:\n",
            "tensor([[[[-0.2416,  0.1972,  0.0812],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.2302]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.1141, -0.0150, -0.1813],\n",
            "          [ 0.0240,  0.1568,  0.1943],\n",
            "          [ 0.1124,  0.0140, -0.0316]]],\n",
            "\n",
            "\n",
            "        [[[-0.0146,  0.1459, -0.1749],\n",
            "          [-0.1287,  0.1655, -0.1921],\n",
            "          [ 0.0227, -0.1735,  0.2135]]],\n",
            "\n",
            "\n",
            "        [[[-0.1326,  0.0777, -0.1734],\n",
            "          [ 0.2746, -0.0787,  0.1938],\n",
            "          [ 0.2148,  0.1972,  0.0917]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.1422, -0.1531],\n",
            "          [-0.2701,  0.0163,  0.1772],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
            "tensor([ 0.1209,  0.3181,  0.0193, -0.2359,  0.2840, -0.2947],\n",
            "       requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp8sek1D2yZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "c6a1ac03-45b1-4dec-fcb8-7a4ae7e13890"
      },
      "source": [
        "# Pruning Mask Prior to removing the re-parametrization\n",
        "print(list(module.named_buffers()))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_mask', tensor([[[[0., 1., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 1.]]]])), ('bias_mask', tensor([0., 1., 0., 0., 1., 1.]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUy1ecqG29Ln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "8f7bd471-56cc-4eb0-a522-0468f109a150"
      },
      "source": [
        "# Pruned Layer Weights before re-parametrization\n",
        "print(module.weight)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.0000,  0.1972,  0.0000],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.0000, -0.1531],\n",
            "          [-0.0000,  0.0163,  0.0000],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYSjZZk63E0y",
        "colab_type": "text"
      },
      "source": [
        "## After removing the Layer re-parametrization Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JBoXw9Y3Sda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "f09dcfde-3d79-4c7d-a3a9-ab1f99d2d499"
      },
      "source": [
        "# Removing the Pruned Weights sets the 'weight_orig' = 'weight'\n",
        "# where,\n",
        "# weight_orig: un-pruned layer parameters\n",
        "# weight: pruned layer parameters\n",
        "prune.remove(module, 'weight')\n",
        "\n",
        "# Note how 'weight_orig' values change to that of 'weight'\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias_orig', Parameter containing:\n",
            "tensor([ 0.1209,  0.3181,  0.0193, -0.2359,  0.2840, -0.2947],\n",
            "       requires_grad=True)), ('weight', Parameter containing:\n",
            "tensor([[[[-0.0000,  0.1972,  0.0000],\n",
            "          [ 0.2535,  0.1136, -0.3117],\n",
            "          [ 0.2692, -0.0886, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.2093, -0.2896,  0.0456],\n",
            "          [-0.0730, -0.0114,  0.2277],\n",
            "          [ 0.1133,  0.3246, -0.0525]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1897,  0.0000, -0.1531],\n",
            "          [-0.0000,  0.0163,  0.0000],\n",
            "          [ 0.2436, -0.1219, -0.3129]]]], requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgGkeskp3uWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06ec23d2-dfa6-493f-bcd8-299e65477b34"
      },
      "source": [
        "# After Layer re-parametrization, the Pruning Mask remains the same\n",
        "print(list(module.named_buffers()))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias_mask', tensor([0., 1., 0., 0., 1., 1.]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "len1VIpWdZtU",
        "colab_type": "text"
      },
      "source": [
        "# Pruning Multiple Parameters in a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBymFJvjdZFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b30ed8e-b1a7-456b-f420-75218df55926"
      },
      "source": [
        "# Instantiate the Model\n",
        "new_model = LeNet()\n",
        "\n",
        "for name, module in new_model.named_modules():\n",
        "  # Prune the Conv2D layer weights by 20%\n",
        "  if isinstance(module, torch.nn.Conv2d):\n",
        "    prune.l1_unstructured(module=module, name='weight', amount=0.2)\n",
        "  \n",
        "  # Prune the Fully Connected layer weights by 40%\n",
        "  if isinstance(module, torch.nn.Linear):\n",
        "    prune.l1_unstructured(module=module, name='weight', amount=0.4)\n",
        "\n",
        "# Verify that all masks exist\n",
        "print(dict(new_model.named_buffers()).keys())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9yidaYIbu4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "3ea5443b-7df8-47a5-f6a1-268e563781c3"
      },
      "source": [
        "# See that if the name of weight changed to weight_orig after pruning\n",
        "module1 = new_model.conv1\n",
        "print(list(module1.named_parameters()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([ 0.2191, -0.2709, -0.1657, -0.1700, -0.1311,  0.1843],\n",
            "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[[[-0.1706,  0.2018,  0.0024],\n",
            "          [-0.1397,  0.2210, -0.2712],\n",
            "          [ 0.0319, -0.1529,  0.1179]]],\n",
            "\n",
            "\n",
            "        [[[-0.2569, -0.1396, -0.3133],\n",
            "          [-0.3023, -0.1992,  0.2522],\n",
            "          [-0.1212, -0.2803, -0.0088]]],\n",
            "\n",
            "\n",
            "        [[[-0.1832, -0.1292,  0.2746],\n",
            "          [-0.2691, -0.2559,  0.2355],\n",
            "          [-0.0645, -0.0838, -0.2616]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3041,  0.2713, -0.1414],\n",
            "          [-0.2913, -0.0827, -0.1309],\n",
            "          [-0.2568,  0.0478, -0.0426]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1748,  0.3034, -0.1001],\n",
            "          [-0.0638, -0.0835,  0.1669],\n",
            "          [ 0.0770,  0.1859, -0.1979]]],\n",
            "\n",
            "\n",
            "        [[[-0.2213, -0.1200, -0.2784],\n",
            "          [ 0.1652,  0.3087,  0.2424],\n",
            "          [-0.0592, -0.1538,  0.2409]]]], requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG5oRn5Oeatv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "47b48bb5-9216-41c7-ef25-28b7322b2252"
      },
      "source": [
        "# Pruned Weights for Module1\n",
        "print(module1.weight)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.1706,  0.2018,  0.0000],\n",
            "          [-0.1397,  0.2210, -0.2712],\n",
            "          [ 0.0000, -0.1529,  0.1179]]],\n",
            "\n",
            "\n",
            "        [[[-0.2569, -0.1396, -0.3133],\n",
            "          [-0.3023, -0.1992,  0.2522],\n",
            "          [-0.1212, -0.2803, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.1832, -0.1292,  0.2746],\n",
            "          [-0.2691, -0.2559,  0.2355],\n",
            "          [-0.0000, -0.0838, -0.2616]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3041,  0.2713, -0.1414],\n",
            "          [-0.2913, -0.0000, -0.1309],\n",
            "          [-0.2568,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1748,  0.3034, -0.1001],\n",
            "          [-0.0000, -0.0000,  0.1669],\n",
            "          [ 0.0000,  0.1859, -0.1979]]],\n",
            "\n",
            "\n",
            "        [[[-0.2213, -0.1200, -0.2784],\n",
            "          [ 0.1652,  0.3087,  0.2424],\n",
            "          [-0.0000, -0.1538,  0.2409]]]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysGgvtXAfx08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79e34ef0-4262-4feb-ebc4-47a94f848a53"
      },
      "source": [
        "# Check the same for Fully Connected Layer\n",
        "module5 = new_model.fc3\n",
        "print(list(module5.named_parameters()))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([ 0.0073,  0.0726, -0.0494, -0.0478,  0.0205,  0.0878, -0.0457,  0.0783,\n",
            "        -0.0423, -0.0953], requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[-0.0121,  0.0096,  0.0407,  0.0687, -0.0731, -0.0717,  0.0151, -0.0051,\n",
            "          0.0559,  0.0077, -0.0048,  0.1006, -0.0579, -0.0172, -0.0556, -0.0644,\n",
            "          0.0444, -0.0422, -0.0488, -0.0573,  0.0707, -0.0927,  0.0407,  0.0150,\n",
            "          0.1050,  0.0166, -0.0963, -0.0201,  0.0359, -0.0087, -0.0280,  0.0911,\n",
            "         -0.0363,  0.0627, -0.0203, -0.0232, -0.0317, -0.1035,  0.0670,  0.0689,\n",
            "          0.0956,  0.0087,  0.0932,  0.0392, -0.0263, -0.0063,  0.0969,  0.0067,\n",
            "         -0.0283, -0.0926, -0.1000,  0.0536,  0.1029,  0.0457, -0.0223, -0.0838,\n",
            "         -0.0846, -0.0883, -0.0229, -0.0800, -0.0601, -0.0663,  0.0160, -0.0242,\n",
            "         -0.0375,  0.0216, -0.0981, -0.0271,  0.0756, -0.0685, -0.0093, -0.0656,\n",
            "          0.0446, -0.0808,  0.0861, -0.0299,  0.0736, -0.0383, -0.1009,  0.0430,\n",
            "          0.0167, -0.0221, -0.0419, -0.0808],\n",
            "        [ 0.1074,  0.0989, -0.0910, -0.1057, -0.0538,  0.0886, -0.0662,  0.0798,\n",
            "          0.0652,  0.0880, -0.0934, -0.0956,  0.0024, -0.0769,  0.0693,  0.0533,\n",
            "          0.0969,  0.0010,  0.0155, -0.0010, -0.0267,  0.0722,  0.0879,  0.0558,\n",
            "          0.0192,  0.0228,  0.0363, -0.0211,  0.0493,  0.0998,  0.1042,  0.0017,\n",
            "          0.0723, -0.0973, -0.0979, -0.0160,  0.0500,  0.0045, -0.0051,  0.0016,\n",
            "          0.0134,  0.0730,  0.0139,  0.0386, -0.0648,  0.0977, -0.0108,  0.0960,\n",
            "          0.0975,  0.0086, -0.0729, -0.0524,  0.0923, -0.0273,  0.1000,  0.0094,\n",
            "          0.0742,  0.0703, -0.0574, -0.0934,  0.0885, -0.0246, -0.0838, -0.0134,\n",
            "          0.0748,  0.0102,  0.0236,  0.0054, -0.0478, -0.0603, -0.0333, -0.0716,\n",
            "          0.0796,  0.0181,  0.0428,  0.0350,  0.1056, -0.0411,  0.0307,  0.0193,\n",
            "          0.0326, -0.1061,  0.0203, -0.0712],\n",
            "        [ 0.1059, -0.0447, -0.0642, -0.0466, -0.0688,  0.0805,  0.0079, -0.0438,\n",
            "         -0.0224, -0.0543,  0.0757,  0.0657, -0.1012,  0.0032,  0.0464,  0.0671,\n",
            "         -0.0894, -0.0346,  0.0908, -0.0729, -0.0211, -0.0930, -0.0921,  0.0140,\n",
            "          0.0347,  0.0180, -0.0807, -0.1088,  0.0013,  0.0142, -0.0971, -0.0756,\n",
            "          0.0023, -0.0525,  0.0501,  0.0358, -0.0223,  0.0179,  0.0881,  0.0196,\n",
            "         -0.0358, -0.0193,  0.1084, -0.0543, -0.0826, -0.0231, -0.0982, -0.1081,\n",
            "         -0.0442,  0.0632,  0.0967, -0.0468, -0.0247,  0.0356,  0.0981, -0.0026,\n",
            "          0.0211,  0.0443, -0.0690, -0.0869,  0.0235,  0.0481,  0.0306, -0.0243,\n",
            "         -0.0959, -0.0422,  0.0837,  0.0364,  0.0855, -0.0280,  0.1059,  0.0099,\n",
            "          0.0099,  0.1043, -0.0288, -0.0339, -0.0074,  0.0267,  0.0456, -0.0514,\n",
            "         -0.0820, -0.0406,  0.0114,  0.0488],\n",
            "        [-0.0349,  0.0735, -0.0155,  0.0143, -0.0332,  0.0074,  0.0851, -0.0276,\n",
            "          0.0211, -0.1089, -0.0532,  0.0554,  0.0987,  0.0786,  0.0583, -0.0444,\n",
            "          0.0747,  0.0595,  0.0709,  0.0391, -0.0130,  0.0221, -0.0109, -0.0824,\n",
            "          0.0875, -0.0538,  0.0528, -0.1035,  0.0740, -0.0503,  0.0874, -0.0131,\n",
            "         -0.0046, -0.0331, -0.0300, -0.0395,  0.0950,  0.0135,  0.0374, -0.1064,\n",
            "          0.0806,  0.0002,  0.0792,  0.0165, -0.0884, -0.0086,  0.1010,  0.0181,\n",
            "         -0.0243,  0.0675, -0.0524,  0.0878, -0.1068, -0.0713,  0.0604,  0.0045,\n",
            "         -0.0838,  0.0555, -0.0448, -0.0409, -0.0034, -0.0478, -0.0269,  0.0853,\n",
            "         -0.0177, -0.0153,  0.0108,  0.0185,  0.0214,  0.1080,  0.0963,  0.0090,\n",
            "         -0.1000, -0.0603, -0.0400,  0.0526, -0.1075,  0.0482, -0.1089, -0.0772,\n",
            "          0.0510, -0.0508,  0.0139,  0.0249],\n",
            "        [ 0.0776,  0.1064,  0.0139,  0.0160, -0.0476, -0.0007, -0.0396, -0.0916,\n",
            "         -0.0807,  0.0076,  0.0121, -0.0699,  0.0975,  0.1026, -0.0999,  0.0298,\n",
            "          0.1001, -0.0231,  0.0298, -0.1017,  0.0019, -0.1057, -0.0188,  0.0992,\n",
            "         -0.0272, -0.0536,  0.0355, -0.0066,  0.0258, -0.0023, -0.0584,  0.0447,\n",
            "          0.0111, -0.0909, -0.0153,  0.0619, -0.0531,  0.0558,  0.0353, -0.1023,\n",
            "         -0.0847,  0.0211,  0.0221,  0.0236,  0.0992, -0.0989,  0.0660,  0.0237,\n",
            "         -0.0629, -0.0358,  0.0204,  0.0155, -0.0708,  0.0925,  0.0606,  0.0347,\n",
            "         -0.0066,  0.0751, -0.0426, -0.0974,  0.0515,  0.0899,  0.0909, -0.0193,\n",
            "          0.0796, -0.0463,  0.0491,  0.0245,  0.0413, -0.0436, -0.0210, -0.0952,\n",
            "         -0.0013,  0.0650,  0.0238,  0.0592,  0.0185,  0.0686,  0.0493,  0.0724,\n",
            "          0.0849,  0.0288, -0.0312,  0.1067],\n",
            "        [ 0.0240, -0.0661, -0.0120,  0.0518, -0.0986, -0.0166,  0.0546, -0.0379,\n",
            "         -0.0915,  0.1040, -0.0855,  0.0912, -0.0515,  0.0791,  0.0823, -0.0924,\n",
            "          0.0335, -0.0914,  0.0235, -0.0961, -0.1013, -0.0702, -0.0537, -0.1038,\n",
            "         -0.1067,  0.0728, -0.0333,  0.0554,  0.0775, -0.1085,  0.0219,  0.0739,\n",
            "          0.0121,  0.0223,  0.0942,  0.0767,  0.1026,  0.0093,  0.0645, -0.1034,\n",
            "          0.0125,  0.0866,  0.1091,  0.0135, -0.0653, -0.1057, -0.0296, -0.0074,\n",
            "          0.0673,  0.0783, -0.0302,  0.0478,  0.0316,  0.0678, -0.1084,  0.0259,\n",
            "         -0.0430, -0.0233,  0.0298, -0.0364, -0.0300, -0.0850, -0.0230, -0.0103,\n",
            "         -0.0270,  0.0304, -0.0055,  0.0972,  0.0942,  0.0662, -0.0529,  0.0403,\n",
            "          0.0262, -0.0766, -0.0955,  0.0318, -0.0392, -0.1065,  0.0765,  0.0131,\n",
            "         -0.0403, -0.1005, -0.0700,  0.0311],\n",
            "        [ 0.0028,  0.0853,  0.0384, -0.0353, -0.0260, -0.0655,  0.0833, -0.0674,\n",
            "          0.0342, -0.1045,  0.0545,  0.0801, -0.0314,  0.0811,  0.0408,  0.0159,\n",
            "         -0.0148,  0.0815,  0.0368, -0.0005,  0.0559,  0.0617, -0.0072, -0.0040,\n",
            "         -0.0367,  0.1008, -0.0619,  0.0383,  0.0763,  0.0035, -0.1040,  0.0682,\n",
            "         -0.1037,  0.0880,  0.0865,  0.0307, -0.0822, -0.0362, -0.0311, -0.0519,\n",
            "         -0.0098,  0.1022,  0.0522,  0.0915, -0.0456, -0.0347, -0.0732, -0.0498,\n",
            "         -0.0652, -0.0826,  0.0186,  0.0099, -0.0405,  0.0971,  0.0653,  0.0805,\n",
            "          0.0547,  0.1047, -0.0574, -0.0808, -0.0192, -0.0758,  0.0739,  0.0861,\n",
            "          0.0638,  0.0431, -0.0626, -0.0553,  0.0656, -0.0328, -0.0500, -0.0167,\n",
            "          0.0317, -0.0110,  0.0330,  0.0878, -0.0165, -0.0372, -0.0918,  0.0782,\n",
            "          0.0778, -0.0013, -0.0473, -0.0403],\n",
            "        [-0.0806,  0.0178,  0.0756,  0.0359, -0.0711,  0.0609, -0.0743, -0.0063,\n",
            "         -0.0405,  0.0854, -0.0236, -0.0176,  0.0224, -0.0661, -0.0800,  0.0703,\n",
            "          0.0547, -0.0033, -0.0685,  0.0897, -0.0816,  0.0785, -0.0855,  0.0741,\n",
            "          0.0035,  0.0791,  0.0060,  0.0315,  0.0327, -0.0339, -0.0779,  0.0502,\n",
            "         -0.0957, -0.0095,  0.0044, -0.0696, -0.0326,  0.0421, -0.0526, -0.0666,\n",
            "         -0.0928, -0.0491, -0.1027, -0.0332,  0.0968,  0.0247,  0.0610,  0.0072,\n",
            "          0.0636, -0.0162,  0.0264, -0.1054, -0.0102, -0.0287, -0.0270,  0.0801,\n",
            "          0.0912,  0.0649, -0.0992, -0.0261, -0.0147, -0.0666, -0.0576,  0.0455,\n",
            "          0.0511, -0.0166, -0.1000,  0.0770,  0.0851,  0.0837, -0.0864,  0.0648,\n",
            "          0.0474, -0.0279,  0.0348, -0.0577,  0.0102,  0.0268,  0.0478,  0.0635,\n",
            "          0.0977, -0.0666, -0.1025,  0.0226],\n",
            "        [-0.0895, -0.0138,  0.0694, -0.0268, -0.0118, -0.0557,  0.1021, -0.0787,\n",
            "          0.0905,  0.1062, -0.0387,  0.0344,  0.0837,  0.0474,  0.0870,  0.0991,\n",
            "         -0.0504,  0.0816,  0.1050,  0.0988,  0.0824,  0.0289, -0.0168, -0.0920,\n",
            "          0.0287,  0.0464, -0.0966, -0.0589, -0.0364,  0.0673,  0.0491, -0.0793,\n",
            "          0.0375,  0.0889,  0.0565,  0.1058, -0.0419, -0.1049, -0.0264,  0.0503,\n",
            "          0.0334, -0.0748,  0.0901, -0.0606, -0.0675, -0.0172,  0.0400,  0.1078,\n",
            "          0.0290,  0.0373, -0.0417,  0.0356,  0.0080, -0.1047, -0.0798, -0.0480,\n",
            "          0.0935,  0.0325, -0.1038, -0.0261, -0.0933, -0.0764, -0.0427, -0.0413,\n",
            "          0.0982,  0.0055,  0.0755, -0.0530,  0.0867,  0.0717,  0.0506, -0.0974,\n",
            "         -0.0931,  0.0032, -0.0231, -0.0029,  0.1051,  0.0024, -0.0010, -0.0054,\n",
            "          0.1031,  0.0621, -0.0380, -0.1062],\n",
            "        [-0.0320, -0.0995,  0.0698,  0.0715, -0.0666, -0.0050,  0.0721, -0.0834,\n",
            "          0.0502,  0.0212, -0.0874, -0.0739,  0.0597,  0.0222,  0.0768,  0.0746,\n",
            "          0.0460, -0.0785, -0.0576,  0.0229,  0.0729,  0.0205, -0.0105,  0.0463,\n",
            "          0.0006,  0.0122, -0.0477,  0.0062, -0.1005,  0.0006, -0.0059, -0.0071,\n",
            "          0.0493, -0.0221,  0.0773,  0.0572, -0.0923,  0.0290,  0.0106, -0.0675,\n",
            "          0.0516,  0.0087,  0.1039,  0.0042,  0.1036, -0.0358, -0.0081, -0.0350,\n",
            "         -0.0391, -0.0472, -0.0087,  0.0716,  0.0752,  0.0264, -0.0342, -0.0500,\n",
            "          0.0602,  0.0597, -0.0931,  0.0241,  0.0388,  0.0656, -0.0278, -0.0094,\n",
            "         -0.0540, -0.0860, -0.0301,  0.0836, -0.0204, -0.0021, -0.0240,  0.0544,\n",
            "         -0.0937,  0.0736,  0.0150,  0.0030,  0.0900,  0.0257,  0.0709,  0.0416,\n",
            "          0.0139, -0.0143,  0.0548, -0.0773]], requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_f0hHETgDbx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "183ef780-bdc9-4565-9156-f3953367f3f4"
      },
      "source": [
        "# Pruned Weights for Module5 i.e. last FC layer\n",
        "print(module5.weight)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0000,  0.0000,  0.0407,  0.0687, -0.0731, -0.0717,  0.0000, -0.0000,\n",
            "          0.0559,  0.0000, -0.0000,  0.1006, -0.0579, -0.0000, -0.0556, -0.0644,\n",
            "          0.0444, -0.0422, -0.0488, -0.0573,  0.0707, -0.0927,  0.0407,  0.0000,\n",
            "          0.1050,  0.0000, -0.0963, -0.0000,  0.0000, -0.0000, -0.0000,  0.0911,\n",
            "         -0.0000,  0.0627, -0.0000, -0.0000, -0.0000, -0.1035,  0.0670,  0.0689,\n",
            "          0.0956,  0.0000,  0.0932,  0.0000, -0.0000, -0.0000,  0.0969,  0.0000,\n",
            "         -0.0000, -0.0926, -0.1000,  0.0536,  0.1029,  0.0457, -0.0000, -0.0838,\n",
            "         -0.0846, -0.0883, -0.0000, -0.0800, -0.0601, -0.0663,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000, -0.0981, -0.0000,  0.0756, -0.0685, -0.0000, -0.0656,\n",
            "          0.0446, -0.0808,  0.0861, -0.0000,  0.0736, -0.0000, -0.1009,  0.0430,\n",
            "          0.0000, -0.0000, -0.0419, -0.0808],\n",
            "        [ 0.1074,  0.0989, -0.0910, -0.1057, -0.0538,  0.0886, -0.0662,  0.0798,\n",
            "          0.0652,  0.0880, -0.0934, -0.0956,  0.0000, -0.0769,  0.0693,  0.0533,\n",
            "          0.0969,  0.0000,  0.0000, -0.0000, -0.0000,  0.0722,  0.0879,  0.0558,\n",
            "          0.0000,  0.0000,  0.0000, -0.0000,  0.0493,  0.0998,  0.1042,  0.0000,\n",
            "          0.0723, -0.0973, -0.0979, -0.0000,  0.0500,  0.0000, -0.0000,  0.0000,\n",
            "          0.0000,  0.0730,  0.0000,  0.0000, -0.0648,  0.0977, -0.0000,  0.0960,\n",
            "          0.0975,  0.0000, -0.0729, -0.0524,  0.0923, -0.0000,  0.1000,  0.0000,\n",
            "          0.0742,  0.0703, -0.0574, -0.0934,  0.0885, -0.0000, -0.0838, -0.0000,\n",
            "          0.0748,  0.0000,  0.0000,  0.0000, -0.0478, -0.0603, -0.0000, -0.0716,\n",
            "          0.0796,  0.0000,  0.0428,  0.0000,  0.1056, -0.0411,  0.0000,  0.0000,\n",
            "          0.0000, -0.1061,  0.0000, -0.0712],\n",
            "        [ 0.1059, -0.0447, -0.0642, -0.0466, -0.0688,  0.0805,  0.0000, -0.0438,\n",
            "         -0.0000, -0.0543,  0.0757,  0.0657, -0.1012,  0.0000,  0.0464,  0.0671,\n",
            "         -0.0894, -0.0000,  0.0908, -0.0729, -0.0000, -0.0930, -0.0921,  0.0000,\n",
            "          0.0000,  0.0000, -0.0807, -0.1088,  0.0000,  0.0000, -0.0971, -0.0756,\n",
            "          0.0000, -0.0525,  0.0501,  0.0000, -0.0000,  0.0000,  0.0881,  0.0000,\n",
            "         -0.0000, -0.0000,  0.1084, -0.0543, -0.0826, -0.0000, -0.0982, -0.1081,\n",
            "         -0.0442,  0.0632,  0.0967, -0.0468, -0.0000,  0.0000,  0.0981, -0.0000,\n",
            "          0.0000,  0.0443, -0.0690, -0.0869,  0.0000,  0.0481,  0.0000, -0.0000,\n",
            "         -0.0959, -0.0422,  0.0837,  0.0000,  0.0855, -0.0000,  0.1059,  0.0000,\n",
            "          0.0000,  0.1043, -0.0000, -0.0000, -0.0000,  0.0000,  0.0456, -0.0514,\n",
            "         -0.0820, -0.0406,  0.0000,  0.0488],\n",
            "        [-0.0000,  0.0735, -0.0000,  0.0000, -0.0000,  0.0000,  0.0851, -0.0000,\n",
            "          0.0000, -0.1089, -0.0532,  0.0554,  0.0987,  0.0786,  0.0583, -0.0444,\n",
            "          0.0747,  0.0595,  0.0709,  0.0000, -0.0000,  0.0000, -0.0000, -0.0824,\n",
            "          0.0875, -0.0538,  0.0528, -0.1035,  0.0740, -0.0503,  0.0874, -0.0000,\n",
            "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0950,  0.0000,  0.0000, -0.1064,\n",
            "          0.0806,  0.0000,  0.0792,  0.0000, -0.0884, -0.0000,  0.1010,  0.0000,\n",
            "         -0.0000,  0.0675, -0.0524,  0.0878, -0.1068, -0.0713,  0.0604,  0.0000,\n",
            "         -0.0838,  0.0555, -0.0448, -0.0409, -0.0000, -0.0478, -0.0000,  0.0853,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.1080,  0.0963,  0.0000,\n",
            "         -0.1000, -0.0603, -0.0000,  0.0526, -0.1075,  0.0482, -0.1089, -0.0772,\n",
            "          0.0510, -0.0508,  0.0000,  0.0000],\n",
            "        [ 0.0776,  0.1064,  0.0000,  0.0000, -0.0476, -0.0000, -0.0000, -0.0916,\n",
            "         -0.0807,  0.0000,  0.0000, -0.0699,  0.0975,  0.1026, -0.0999,  0.0000,\n",
            "          0.1001, -0.0000,  0.0000, -0.1017,  0.0000, -0.1057, -0.0000,  0.0992,\n",
            "         -0.0000, -0.0536,  0.0000, -0.0000,  0.0000, -0.0000, -0.0584,  0.0447,\n",
            "          0.0000, -0.0909, -0.0000,  0.0619, -0.0531,  0.0558,  0.0000, -0.1023,\n",
            "         -0.0847,  0.0000,  0.0000,  0.0000,  0.0992, -0.0989,  0.0660,  0.0000,\n",
            "         -0.0629, -0.0000,  0.0000,  0.0000, -0.0708,  0.0925,  0.0606,  0.0000,\n",
            "         -0.0000,  0.0751, -0.0426, -0.0974,  0.0515,  0.0899,  0.0909, -0.0000,\n",
            "          0.0796, -0.0463,  0.0491,  0.0000,  0.0413, -0.0436, -0.0000, -0.0952,\n",
            "         -0.0000,  0.0650,  0.0000,  0.0592,  0.0000,  0.0686,  0.0493,  0.0724,\n",
            "          0.0849,  0.0000, -0.0000,  0.1067],\n",
            "        [ 0.0000, -0.0661, -0.0000,  0.0518, -0.0986, -0.0000,  0.0546, -0.0000,\n",
            "         -0.0915,  0.1040, -0.0855,  0.0912, -0.0515,  0.0791,  0.0823, -0.0924,\n",
            "          0.0000, -0.0914,  0.0000, -0.0961, -0.1013, -0.0702, -0.0537, -0.1038,\n",
            "         -0.1067,  0.0728, -0.0000,  0.0554,  0.0775, -0.1085,  0.0000,  0.0739,\n",
            "          0.0000,  0.0000,  0.0942,  0.0767,  0.1026,  0.0000,  0.0645, -0.1034,\n",
            "          0.0000,  0.0866,  0.1091,  0.0000, -0.0653, -0.1057, -0.0000, -0.0000,\n",
            "          0.0673,  0.0783, -0.0000,  0.0478,  0.0000,  0.0678, -0.1084,  0.0000,\n",
            "         -0.0430, -0.0000,  0.0000, -0.0000, -0.0000, -0.0850, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000, -0.0000,  0.0972,  0.0942,  0.0662, -0.0529,  0.0000,\n",
            "          0.0000, -0.0766, -0.0955,  0.0000, -0.0000, -0.1065,  0.0765,  0.0000,\n",
            "         -0.0000, -0.1005, -0.0700,  0.0000],\n",
            "        [ 0.0000,  0.0853,  0.0000, -0.0000, -0.0000, -0.0655,  0.0833, -0.0674,\n",
            "          0.0000, -0.1045,  0.0545,  0.0801, -0.0000,  0.0811,  0.0408,  0.0000,\n",
            "         -0.0000,  0.0815,  0.0000, -0.0000,  0.0559,  0.0617, -0.0000, -0.0000,\n",
            "         -0.0000,  0.1008, -0.0619,  0.0000,  0.0763,  0.0000, -0.1040,  0.0682,\n",
            "         -0.1037,  0.0880,  0.0865,  0.0000, -0.0822, -0.0000, -0.0000, -0.0519,\n",
            "         -0.0000,  0.1022,  0.0522,  0.0915, -0.0456, -0.0000, -0.0732, -0.0498,\n",
            "         -0.0652, -0.0826,  0.0000,  0.0000, -0.0405,  0.0971,  0.0653,  0.0805,\n",
            "          0.0547,  0.1047, -0.0574, -0.0808, -0.0000, -0.0758,  0.0739,  0.0861,\n",
            "          0.0638,  0.0431, -0.0626, -0.0553,  0.0656, -0.0000, -0.0500, -0.0000,\n",
            "          0.0000, -0.0000,  0.0000,  0.0878, -0.0000, -0.0000, -0.0918,  0.0782,\n",
            "          0.0778, -0.0000, -0.0473, -0.0000],\n",
            "        [-0.0806,  0.0000,  0.0756,  0.0000, -0.0711,  0.0609, -0.0743, -0.0000,\n",
            "         -0.0405,  0.0854, -0.0000, -0.0000,  0.0000, -0.0661, -0.0800,  0.0703,\n",
            "          0.0547, -0.0000, -0.0685,  0.0897, -0.0816,  0.0785, -0.0855,  0.0741,\n",
            "          0.0000,  0.0791,  0.0000,  0.0000,  0.0000, -0.0000, -0.0779,  0.0502,\n",
            "         -0.0957, -0.0000,  0.0000, -0.0696, -0.0000,  0.0421, -0.0526, -0.0666,\n",
            "         -0.0928, -0.0491, -0.1027, -0.0000,  0.0968,  0.0000,  0.0610,  0.0000,\n",
            "          0.0636, -0.0000,  0.0000, -0.1054, -0.0000, -0.0000, -0.0000,  0.0801,\n",
            "          0.0912,  0.0649, -0.0992, -0.0000, -0.0000, -0.0666, -0.0576,  0.0455,\n",
            "          0.0511, -0.0000, -0.1000,  0.0770,  0.0851,  0.0837, -0.0864,  0.0648,\n",
            "          0.0474, -0.0000,  0.0000, -0.0577,  0.0000,  0.0000,  0.0478,  0.0635,\n",
            "          0.0977, -0.0666, -0.1025,  0.0000],\n",
            "        [-0.0895, -0.0000,  0.0694, -0.0000, -0.0000, -0.0557,  0.1021, -0.0787,\n",
            "          0.0905,  0.1062, -0.0000,  0.0000,  0.0837,  0.0474,  0.0870,  0.0991,\n",
            "         -0.0504,  0.0816,  0.1050,  0.0988,  0.0824,  0.0000, -0.0000, -0.0920,\n",
            "          0.0000,  0.0464, -0.0966, -0.0589, -0.0000,  0.0673,  0.0491, -0.0793,\n",
            "          0.0000,  0.0889,  0.0565,  0.1058, -0.0419, -0.1049, -0.0000,  0.0503,\n",
            "          0.0000, -0.0748,  0.0901, -0.0606, -0.0675, -0.0000,  0.0000,  0.1078,\n",
            "          0.0000,  0.0000, -0.0417,  0.0000,  0.0000, -0.1047, -0.0798, -0.0480,\n",
            "          0.0935,  0.0000, -0.1038, -0.0000, -0.0933, -0.0764, -0.0427, -0.0413,\n",
            "          0.0982,  0.0000,  0.0755, -0.0530,  0.0867,  0.0717,  0.0506, -0.0974,\n",
            "         -0.0931,  0.0000, -0.0000, -0.0000,  0.1051,  0.0000, -0.0000, -0.0000,\n",
            "          0.1031,  0.0621, -0.0000, -0.1062],\n",
            "        [-0.0000, -0.0995,  0.0698,  0.0715, -0.0666, -0.0000,  0.0721, -0.0834,\n",
            "          0.0502,  0.0000, -0.0874, -0.0739,  0.0597,  0.0000,  0.0768,  0.0746,\n",
            "          0.0460, -0.0785, -0.0576,  0.0000,  0.0729,  0.0000, -0.0000,  0.0463,\n",
            "          0.0000,  0.0000, -0.0477,  0.0000, -0.1005,  0.0000, -0.0000, -0.0000,\n",
            "          0.0493, -0.0000,  0.0773,  0.0572, -0.0923,  0.0000,  0.0000, -0.0675,\n",
            "          0.0516,  0.0000,  0.1039,  0.0000,  0.1036, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0472, -0.0000,  0.0716,  0.0752,  0.0000, -0.0000, -0.0500,\n",
            "          0.0602,  0.0597, -0.0931,  0.0000,  0.0000,  0.0656, -0.0000, -0.0000,\n",
            "         -0.0540, -0.0860, -0.0000,  0.0836, -0.0000, -0.0000, -0.0000,  0.0544,\n",
            "         -0.0937,  0.0736,  0.0000,  0.0000,  0.0900,  0.0000,  0.0709,  0.0416,\n",
            "          0.0000, -0.0000,  0.0548, -0.0773]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA9y8KzHRZS2",
        "colab_type": "text"
      },
      "source": [
        "# Global Pruning\n",
        "\n",
        "In Global Pruning, we **prune the model all at once**, by removing (for example) the lowest 20% of connections across the whole model, instead of removing the lowest 20% of connections in each layer. \n",
        "\n",
        "This is likely to result in different pruning percentages per layer and we use it using **global_unstructured** from **torch.nn.utils.prune**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3OsfQMNRYkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the Model\n",
        "model = LeNet()\n",
        "\n",
        "# Define the Layers and their Parameters (weight/bias) to Prune\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "# Prun the whole model Globally i.e.\n",
        "# Assign same Percentage of pruning for all layers instead of defining\n",
        "# Pruning percentages per layer.\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    # 20% pruninig across all layers combined\n",
        "    amount=0.2,\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRKKTgyoS1q4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2e2a396d-919f-4bf2-cd3e-bbc1d0a99687"
      },
      "source": [
        "model"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LeNet(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3fTDroHS2yX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39d90a77-89fc-4171-80bf-122d28c6c152"
      },
      "source": [
        "# Check all Model layers Original Values\n",
        "# All layers show 'weight_orig' as the model has been pruned for weight\n",
        "# For name and params in all model named_parameters\n",
        "for name, param in model.named_parameters():\n",
        "  # If the parameter is updatable i.e. requires_grad = True\n",
        "  if param.requires_grad:\n",
        "    # Print the Layer Name and it's Parameters\n",
        "    print (name, param.data)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias tensor([-0.1796, -0.1487, -0.1289,  0.0827,  0.0039, -0.1429])\n",
            "conv1.weight_orig tensor([[[[ 0.2952, -0.0966, -0.1436],\n",
            "          [-0.3235, -0.1454, -0.0882],\n",
            "          [-0.1662,  0.0028,  0.0075]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0438,  0.2900,  0.2357],\n",
            "          [ 0.1284, -0.1668,  0.2187],\n",
            "          [ 0.1519,  0.0483,  0.0010]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0661, -0.1535,  0.2512],\n",
            "          [-0.0804, -0.1958, -0.1135],\n",
            "          [ 0.1595,  0.3300, -0.0040]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2241, -0.2220,  0.2834],\n",
            "          [ 0.1820,  0.3208,  0.0883],\n",
            "          [ 0.0850, -0.3110,  0.1855]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0314,  0.1655, -0.1768],\n",
            "          [-0.3055,  0.1989,  0.0736],\n",
            "          [ 0.2201,  0.2403, -0.1500]]],\n",
            "\n",
            "\n",
            "        [[[-0.1379, -0.2519, -0.0242],\n",
            "          [ 0.3203,  0.1620, -0.1227],\n",
            "          [-0.1037, -0.2465,  0.2425]]]])\n",
            "conv2.bias tensor([-0.1147,  0.0860, -0.1281, -0.0708,  0.1334, -0.0463,  0.0458,  0.0561,\n",
            "         0.0008,  0.1178, -0.0454,  0.0139, -0.0689,  0.0034,  0.0701,  0.0936])\n",
            "conv2.weight_orig tensor([[[[-0.0522, -0.0653,  0.1076],\n",
            "          [ 0.0472, -0.0032, -0.1190],\n",
            "          [-0.0191,  0.0730, -0.0169]],\n",
            "\n",
            "         [[-0.1157, -0.0502,  0.0126],\n",
            "          [ 0.0253,  0.0563,  0.1289],\n",
            "          [ 0.0600, -0.0425,  0.0960]],\n",
            "\n",
            "         [[-0.0588, -0.0693, -0.0779],\n",
            "          [ 0.0692, -0.0597, -0.0599],\n",
            "          [ 0.0073, -0.0131, -0.1052]],\n",
            "\n",
            "         [[-0.0724,  0.0783, -0.0744],\n",
            "          [ 0.0748,  0.1295,  0.0356],\n",
            "          [-0.1147,  0.1182, -0.0845]],\n",
            "\n",
            "         [[ 0.0973, -0.0684,  0.1240],\n",
            "          [-0.1211, -0.0096, -0.0103],\n",
            "          [-0.0466,  0.0321, -0.1346]],\n",
            "\n",
            "         [[ 0.1150,  0.0987, -0.0103],\n",
            "          [-0.1228,  0.1151, -0.1207],\n",
            "          [ 0.0867,  0.0743,  0.0301]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0023, -0.1236,  0.0196],\n",
            "          [-0.0226, -0.0653,  0.0714],\n",
            "          [ 0.0982,  0.1152, -0.0117]],\n",
            "\n",
            "         [[ 0.0500, -0.1290, -0.0602],\n",
            "          [-0.1071, -0.0929,  0.0873],\n",
            "          [ 0.0713, -0.0453,  0.1210]],\n",
            "\n",
            "         [[-0.0873,  0.1051, -0.1342],\n",
            "          [ 0.1067,  0.0246, -0.0586],\n",
            "          [ 0.0312, -0.0722, -0.0848]],\n",
            "\n",
            "         [[-0.0840,  0.1323,  0.1144],\n",
            "          [ 0.0138, -0.0481, -0.0718],\n",
            "          [ 0.0700,  0.0155,  0.0889]],\n",
            "\n",
            "         [[-0.0039,  0.0172, -0.0624],\n",
            "          [-0.1109, -0.0872,  0.1230],\n",
            "          [-0.0363,  0.0552, -0.0536]],\n",
            "\n",
            "         [[-0.0097,  0.1333, -0.0136],\n",
            "          [ 0.0382, -0.0052,  0.0042],\n",
            "          [-0.0423,  0.0526, -0.0378]]],\n",
            "\n",
            "\n",
            "        [[[-0.0667, -0.1304,  0.0445],\n",
            "          [ 0.0805, -0.1330,  0.0378],\n",
            "          [ 0.0024, -0.0053, -0.1292]],\n",
            "\n",
            "         [[ 0.0318, -0.0725, -0.0107],\n",
            "          [ 0.0082, -0.0122,  0.0778],\n",
            "          [ 0.1136, -0.0129,  0.1130]],\n",
            "\n",
            "         [[-0.0155, -0.0941, -0.0728],\n",
            "          [-0.0692,  0.0630, -0.0634],\n",
            "          [-0.1351,  0.0418,  0.0122]],\n",
            "\n",
            "         [[-0.0978, -0.0267, -0.0355],\n",
            "          [ 0.0845, -0.0836,  0.0328],\n",
            "          [ 0.1155, -0.1228,  0.0789]],\n",
            "\n",
            "         [[ 0.1188,  0.0177, -0.1118],\n",
            "          [-0.0820, -0.0859, -0.1075],\n",
            "          [-0.1288, -0.0850,  0.0170]],\n",
            "\n",
            "         [[ 0.0807, -0.0597, -0.0008],\n",
            "          [ 0.1029, -0.0449, -0.1327],\n",
            "          [ 0.0340,  0.0718,  0.0906]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0054,  0.0764, -0.0441],\n",
            "          [-0.0260, -0.0423, -0.1222],\n",
            "          [-0.1083, -0.0052, -0.0883]],\n",
            "\n",
            "         [[ 0.0143,  0.0477, -0.1268],\n",
            "          [-0.0989,  0.1287, -0.0319],\n",
            "          [-0.1057, -0.0121,  0.0502]],\n",
            "\n",
            "         [[-0.0526,  0.0070,  0.0036],\n",
            "          [ 0.1321,  0.1114, -0.0096],\n",
            "          [ 0.0028, -0.0545, -0.1288]],\n",
            "\n",
            "         [[-0.0612, -0.1045, -0.1120],\n",
            "          [ 0.0558, -0.0281,  0.1306],\n",
            "          [-0.1220,  0.1167, -0.0348]],\n",
            "\n",
            "         [[-0.1072, -0.0704,  0.0361],\n",
            "          [ 0.0368, -0.0982, -0.0838],\n",
            "          [-0.0624,  0.0857,  0.0439]],\n",
            "\n",
            "         [[ 0.1042, -0.0414,  0.0053],\n",
            "          [ 0.0644, -0.1273,  0.1032],\n",
            "          [ 0.0973, -0.0470,  0.0333]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0730, -0.0740, -0.0991],\n",
            "          [-0.1143, -0.0345, -0.1110],\n",
            "          [-0.0007, -0.0124,  0.0268]],\n",
            "\n",
            "         [[ 0.1170, -0.1353, -0.0484],\n",
            "          [-0.0786,  0.0355, -0.0930],\n",
            "          [-0.0183,  0.0294, -0.1302]],\n",
            "\n",
            "         [[ 0.0664, -0.0383, -0.0898],\n",
            "          [ 0.0952,  0.0355, -0.1051],\n",
            "          [-0.0325,  0.0868, -0.1012]],\n",
            "\n",
            "         [[ 0.0651,  0.0111,  0.0461],\n",
            "          [ 0.0796, -0.0524,  0.1114],\n",
            "          [-0.0269, -0.1069,  0.0182]],\n",
            "\n",
            "         [[ 0.0802, -0.0531,  0.0659],\n",
            "          [ 0.0956,  0.0812,  0.0955],\n",
            "          [-0.0128, -0.0354, -0.0738]],\n",
            "\n",
            "         [[ 0.1070, -0.1161, -0.1060],\n",
            "          [-0.1314, -0.0207,  0.0787],\n",
            "          [-0.0412,  0.0896,  0.1007]]],\n",
            "\n",
            "\n",
            "        [[[-0.0501,  0.0707, -0.0813],\n",
            "          [ 0.0186,  0.0092, -0.0742],\n",
            "          [-0.0174,  0.1322, -0.0086]],\n",
            "\n",
            "         [[-0.1257, -0.1002,  0.0358],\n",
            "          [-0.0846, -0.0363,  0.0091],\n",
            "          [-0.0614, -0.0370,  0.0981]],\n",
            "\n",
            "         [[-0.1020,  0.0040, -0.1198],\n",
            "          [ 0.1106, -0.0127, -0.1285],\n",
            "          [-0.0364,  0.1212,  0.0011]],\n",
            "\n",
            "         [[ 0.0414,  0.0299,  0.1107],\n",
            "          [-0.0123,  0.1013,  0.0520],\n",
            "          [-0.0956,  0.0438,  0.0877]],\n",
            "\n",
            "         [[ 0.0477,  0.1129,  0.0580],\n",
            "          [ 0.0789,  0.1100,  0.0347],\n",
            "          [-0.0353, -0.0121,  0.0653]],\n",
            "\n",
            "         [[-0.0317,  0.1332,  0.0454],\n",
            "          [ 0.0003,  0.1331,  0.0835],\n",
            "          [-0.0615,  0.0367,  0.0313]]],\n",
            "\n",
            "\n",
            "        [[[-0.0937,  0.0322,  0.0351],\n",
            "          [ 0.0108,  0.0582, -0.1049],\n",
            "          [ 0.0911, -0.0792,  0.0329]],\n",
            "\n",
            "         [[-0.0161, -0.1311,  0.0098],\n",
            "          [ 0.0314, -0.0744, -0.1338],\n",
            "          [-0.0102,  0.1030, -0.0349]],\n",
            "\n",
            "         [[ 0.1164, -0.0003,  0.0659],\n",
            "          [ 0.0495,  0.1021, -0.0659],\n",
            "          [-0.1075, -0.0943,  0.0614]],\n",
            "\n",
            "         [[-0.0497, -0.1240, -0.1348],\n",
            "          [ 0.1191, -0.0160,  0.0652],\n",
            "          [ 0.0078, -0.1045,  0.1227]],\n",
            "\n",
            "         [[-0.1324,  0.0536,  0.0489],\n",
            "          [ 0.1113,  0.0348, -0.0301],\n",
            "          [ 0.1217,  0.1140, -0.0913]],\n",
            "\n",
            "         [[ 0.0602, -0.0917,  0.0849],\n",
            "          [ 0.0295, -0.0668,  0.1090],\n",
            "          [-0.0943, -0.0447,  0.0946]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1052,  0.1340,  0.0683],\n",
            "          [ 0.0820, -0.0203,  0.0483],\n",
            "          [ 0.0272,  0.0376,  0.1270]],\n",
            "\n",
            "         [[ 0.0492,  0.1084,  0.0849],\n",
            "          [-0.1320,  0.0864,  0.0543],\n",
            "          [ 0.0160, -0.1217,  0.0609]],\n",
            "\n",
            "         [[ 0.0533,  0.0407,  0.0036],\n",
            "          [-0.0445,  0.0377,  0.1055],\n",
            "          [-0.0712,  0.1176, -0.0299]],\n",
            "\n",
            "         [[ 0.0997,  0.0995,  0.0916],\n",
            "          [ 0.0706,  0.1359, -0.0312],\n",
            "          [ 0.0387,  0.0014,  0.0615]],\n",
            "\n",
            "         [[-0.0051, -0.1018, -0.0029],\n",
            "          [-0.0985,  0.0628, -0.1136],\n",
            "          [ 0.0644,  0.0245,  0.1103]],\n",
            "\n",
            "         [[ 0.0331, -0.0350, -0.1116],\n",
            "          [-0.0363,  0.1153,  0.1078],\n",
            "          [-0.0037,  0.0691, -0.1022]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0324,  0.0442, -0.0400],\n",
            "          [ 0.0303,  0.0960,  0.1176],\n",
            "          [ 0.0096, -0.0699,  0.1270]],\n",
            "\n",
            "         [[ 0.0555,  0.0132,  0.0501],\n",
            "          [-0.0362,  0.1323, -0.0552],\n",
            "          [-0.1106, -0.0511, -0.0507]],\n",
            "\n",
            "         [[ 0.0106,  0.0046, -0.0213],\n",
            "          [ 0.0584, -0.1037,  0.1346],\n",
            "          [ 0.1134,  0.0081, -0.0828]],\n",
            "\n",
            "         [[-0.1342,  0.0904,  0.0962],\n",
            "          [-0.0136,  0.0893,  0.0164],\n",
            "          [-0.1048,  0.0076,  0.0590]],\n",
            "\n",
            "         [[ 0.0012,  0.0209,  0.0512],\n",
            "          [-0.0198,  0.0919, -0.0422],\n",
            "          [ 0.1308,  0.0297, -0.0156]],\n",
            "\n",
            "         [[-0.1044, -0.0832, -0.1198],\n",
            "          [ 0.0708,  0.0886,  0.0672],\n",
            "          [-0.0857, -0.0281,  0.0935]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0372,  0.1197,  0.1355],\n",
            "          [-0.1213,  0.0357,  0.0401],\n",
            "          [ 0.0543,  0.0185,  0.0149]],\n",
            "\n",
            "         [[-0.0151, -0.0006,  0.1209],\n",
            "          [ 0.0605, -0.1081,  0.1233],\n",
            "          [ 0.0465,  0.1248,  0.0594]],\n",
            "\n",
            "         [[-0.0588, -0.1303, -0.0907],\n",
            "          [ 0.0582, -0.0192, -0.0323],\n",
            "          [ 0.0682,  0.1173,  0.0033]],\n",
            "\n",
            "         [[-0.1202,  0.0652, -0.0529],\n",
            "          [ 0.1236,  0.0779,  0.0581],\n",
            "          [ 0.0011,  0.0116,  0.1028]],\n",
            "\n",
            "         [[ 0.0234, -0.0459, -0.0908],\n",
            "          [-0.0520, -0.0877, -0.0094],\n",
            "          [-0.1260,  0.0570,  0.0837]],\n",
            "\n",
            "         [[ 0.1224, -0.0527, -0.0393],\n",
            "          [-0.0523, -0.1151,  0.0861],\n",
            "          [-0.0081, -0.0335, -0.0322]]],\n",
            "\n",
            "\n",
            "        [[[-0.0327, -0.0999, -0.0026],\n",
            "          [-0.0706,  0.0409,  0.0552],\n",
            "          [ 0.0305, -0.1125, -0.1214]],\n",
            "\n",
            "         [[ 0.1340, -0.1089,  0.0308],\n",
            "          [-0.0185, -0.1017, -0.0951],\n",
            "          [-0.0725,  0.0771,  0.1240]],\n",
            "\n",
            "         [[-0.0137,  0.1321,  0.0827],\n",
            "          [ 0.0547,  0.1344,  0.0982],\n",
            "          [-0.0554,  0.0446,  0.0533]],\n",
            "\n",
            "         [[-0.0697,  0.0856, -0.1157],\n",
            "          [ 0.1335, -0.0856, -0.0220],\n",
            "          [-0.0426, -0.0023,  0.0249]],\n",
            "\n",
            "         [[-0.0630, -0.0902,  0.1331],\n",
            "          [-0.0604, -0.0955,  0.0458],\n",
            "          [ 0.0295, -0.0771, -0.0210]],\n",
            "\n",
            "         [[-0.0028,  0.1348,  0.0448],\n",
            "          [ 0.1053,  0.0994,  0.0905],\n",
            "          [-0.1021, -0.0102,  0.0139]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1228,  0.1151,  0.1174],\n",
            "          [-0.0544, -0.0332,  0.0561],\n",
            "          [ 0.0346,  0.0998, -0.0039]],\n",
            "\n",
            "         [[ 0.0800,  0.0119, -0.0213],\n",
            "          [ 0.0550,  0.1143,  0.0386],\n",
            "          [ 0.0930, -0.0916,  0.1315]],\n",
            "\n",
            "         [[ 0.1317, -0.0309, -0.1081],\n",
            "          [-0.1193,  0.0826,  0.1059],\n",
            "          [ 0.0575, -0.1221,  0.0043]],\n",
            "\n",
            "         [[ 0.0702, -0.0720, -0.0850],\n",
            "          [-0.0801,  0.1166, -0.0809],\n",
            "          [-0.0969,  0.0243,  0.0369]],\n",
            "\n",
            "         [[ 0.1176,  0.1227, -0.0941],\n",
            "          [ 0.1313, -0.0021,  0.0472],\n",
            "          [-0.0254, -0.1119,  0.0547]],\n",
            "\n",
            "         [[ 0.0078,  0.1050,  0.0391],\n",
            "          [ 0.1039, -0.0809, -0.0810],\n",
            "          [ 0.0054, -0.0296,  0.0055]]],\n",
            "\n",
            "\n",
            "        [[[-0.1068,  0.0430, -0.1187],\n",
            "          [-0.0924, -0.0921,  0.0445],\n",
            "          [-0.1170,  0.0183, -0.1151]],\n",
            "\n",
            "         [[ 0.0833, -0.1121,  0.0700],\n",
            "          [ 0.0495,  0.1288,  0.1264],\n",
            "          [ 0.0725, -0.0008, -0.0542]],\n",
            "\n",
            "         [[-0.1342, -0.0459,  0.1118],\n",
            "          [ 0.1358, -0.0004,  0.0453],\n",
            "          [ 0.1141, -0.1111, -0.1200]],\n",
            "\n",
            "         [[ 0.0413, -0.1063,  0.1320],\n",
            "          [-0.0135,  0.0213,  0.0901],\n",
            "          [ 0.0977, -0.0053, -0.1258]],\n",
            "\n",
            "         [[ 0.0769,  0.1121, -0.0774],\n",
            "          [-0.0409, -0.0977, -0.0715],\n",
            "          [ 0.0962,  0.0661, -0.0043]],\n",
            "\n",
            "         [[ 0.0894, -0.0912,  0.0659],\n",
            "          [ 0.0517, -0.1355, -0.0310],\n",
            "          [ 0.0911,  0.1030, -0.1311]]],\n",
            "\n",
            "\n",
            "        [[[-0.0561,  0.0985, -0.0748],\n",
            "          [ 0.0383,  0.0377, -0.1281],\n",
            "          [-0.0487,  0.1114,  0.0509]],\n",
            "\n",
            "         [[-0.1088, -0.0188, -0.0591],\n",
            "          [-0.1005,  0.0113,  0.0106],\n",
            "          [ 0.1221,  0.0046, -0.1142]],\n",
            "\n",
            "         [[ 0.1250,  0.1149, -0.1202],\n",
            "          [-0.0752, -0.0884,  0.0623],\n",
            "          [-0.0824,  0.1039, -0.0639]],\n",
            "\n",
            "         [[-0.0644,  0.0245, -0.0626],\n",
            "          [ 0.0549,  0.1171,  0.0784],\n",
            "          [-0.0678, -0.0870,  0.0526]],\n",
            "\n",
            "         [[-0.0540,  0.0862,  0.1008],\n",
            "          [ 0.0824, -0.0299,  0.0494],\n",
            "          [ 0.0064, -0.1180,  0.0784]],\n",
            "\n",
            "         [[ 0.0322,  0.0179,  0.0275],\n",
            "          [ 0.0705,  0.0835,  0.0806],\n",
            "          [-0.0246, -0.1063,  0.0062]]],\n",
            "\n",
            "\n",
            "        [[[-0.1317, -0.0049,  0.0115],\n",
            "          [-0.0319, -0.0983, -0.0775],\n",
            "          [-0.0798,  0.0583,  0.0707]],\n",
            "\n",
            "         [[ 0.1073,  0.0016, -0.0220],\n",
            "          [-0.0809,  0.1217,  0.1343],\n",
            "          [ 0.0663,  0.0470, -0.0108]],\n",
            "\n",
            "         [[-0.0131,  0.0298, -0.0465],\n",
            "          [ 0.0030, -0.0787,  0.0328],\n",
            "          [-0.0562, -0.1243, -0.0211]],\n",
            "\n",
            "         [[ 0.0403, -0.0353, -0.0993],\n",
            "          [ 0.0595, -0.1109, -0.0652],\n",
            "          [ 0.1264, -0.0958,  0.0601]],\n",
            "\n",
            "         [[ 0.0098,  0.1361, -0.0127],\n",
            "          [-0.1352, -0.0120, -0.0183],\n",
            "          [-0.1019, -0.0708, -0.0890]],\n",
            "\n",
            "         [[-0.1137,  0.0788,  0.1044],\n",
            "          [ 0.0219,  0.0539,  0.0704],\n",
            "          [-0.0102,  0.0714,  0.0487]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0137,  0.0822,  0.0229],\n",
            "          [-0.0195,  0.1244, -0.0993],\n",
            "          [-0.0725,  0.0043,  0.1202]],\n",
            "\n",
            "         [[-0.0887, -0.0632, -0.1200],\n",
            "          [-0.0862, -0.0931,  0.0663],\n",
            "          [ 0.0640, -0.0399, -0.0291]],\n",
            "\n",
            "         [[ 0.0404,  0.0223, -0.0219],\n",
            "          [ 0.0676,  0.0123,  0.0665],\n",
            "          [-0.0775, -0.1300, -0.0119]],\n",
            "\n",
            "         [[-0.1184, -0.0284, -0.0498],\n",
            "          [ 0.0482,  0.1256, -0.0133],\n",
            "          [ 0.0384, -0.1115, -0.0040]],\n",
            "\n",
            "         [[-0.0894, -0.0508,  0.0797],\n",
            "          [ 0.1151,  0.0842, -0.1172],\n",
            "          [ 0.0002,  0.1145, -0.1101]],\n",
            "\n",
            "         [[-0.0253,  0.0459, -0.0916],\n",
            "          [-0.0382,  0.0927,  0.0230],\n",
            "          [ 0.0461,  0.1093,  0.0248]]]])\n",
            "fc1.bias tensor([ 1.9311e-02, -4.5355e-02, -4.5927e-02, -6.1064e-03,  2.6706e-02,\n",
            "         1.8005e-02, -8.3463e-03, -6.0293e-03, -6.9601e-03, -7.3303e-03,\n",
            "         1.2165e-02,  1.1851e-02,  2.5935e-02,  1.4994e-03, -1.7878e-02,\n",
            "        -4.4667e-02,  7.5522e-03,  4.7738e-03, -1.5252e-02,  3.3368e-02,\n",
            "         1.6182e-03, -1.3099e-02,  3.2675e-02, -2.0259e-02, -4.7632e-04,\n",
            "         2.2938e-02, -1.7866e-02, -2.9888e-02,  4.2661e-02,  3.1333e-03,\n",
            "        -3.8951e-02,  6.5554e-05,  3.8066e-02, -1.7697e-02, -7.6630e-03,\n",
            "        -3.8461e-02, -4.6115e-02, -1.6164e-02, -1.7020e-02,  1.6375e-03,\n",
            "         3.5302e-02, -6.7538e-03, -5.3030e-03, -3.2018e-02, -2.4711e-02,\n",
            "         9.9449e-03,  1.9769e-02, -9.5475e-03,  2.6964e-02, -3.8622e-02,\n",
            "        -3.2909e-03,  2.9586e-02,  2.3664e-02,  3.9279e-03, -1.0585e-02,\n",
            "        -9.9205e-03,  2.2523e-02, -8.3074e-03,  1.7195e-02, -3.3264e-02,\n",
            "        -2.6387e-02,  1.2114e-02, -2.8961e-02,  3.7313e-02, -1.6135e-02,\n",
            "        -4.9444e-02,  1.6692e-02,  4.5590e-02, -3.8588e-02,  3.1339e-02,\n",
            "        -2.6352e-02,  2.6560e-02, -4.3198e-03,  1.6905e-02,  1.9147e-02,\n",
            "        -2.9945e-02,  9.6119e-03, -4.3892e-02, -1.2067e-02,  6.1377e-03,\n",
            "         3.1034e-02, -1.8572e-02,  5.4426e-03, -2.0694e-02, -1.1613e-02,\n",
            "        -3.7470e-02, -3.9460e-02,  9.7014e-03,  4.6215e-02,  1.6619e-03,\n",
            "         2.6970e-02, -7.8471e-03, -3.0141e-02,  3.9007e-02, -1.2830e-02,\n",
            "         3.0129e-03, -2.2233e-02,  4.6116e-02, -1.2495e-02, -3.7263e-02,\n",
            "         1.3438e-03, -4.6171e-02,  3.2419e-02,  7.1516e-03,  1.0319e-02,\n",
            "         3.8661e-03, -3.7670e-03,  4.6768e-02, -2.9860e-02,  3.8634e-02,\n",
            "         1.8610e-02, -2.3191e-02, -8.5595e-03, -4.4495e-02,  3.0522e-02,\n",
            "        -1.6030e-02,  1.4306e-02,  2.2037e-02,  2.7780e-02, -1.3561e-02])\n",
            "fc1.weight_orig tensor([[-0.0009,  0.0045,  0.0263,  ...,  0.0012,  0.0330, -0.0059],\n",
            "        [ 0.0063, -0.0395,  0.0207,  ...,  0.0333, -0.0187,  0.0178],\n",
            "        [-0.0163, -0.0367,  0.0291,  ..., -0.0351, -0.0313,  0.0055],\n",
            "        ...,\n",
            "        [ 0.0446,  0.0065,  0.0391,  ..., -0.0390, -0.0069,  0.0134],\n",
            "        [ 0.0192,  0.0168,  0.0085,  ..., -0.0067,  0.0418, -0.0300],\n",
            "        [ 0.0334,  0.0225, -0.0157,  ...,  0.0141, -0.0384, -0.0137]])\n",
            "fc2.bias tensor([ 0.0821, -0.0263, -0.0328,  0.0582,  0.0433,  0.0686, -0.0499,  0.0206,\n",
            "        -0.0745, -0.0575,  0.0297, -0.0369,  0.0523,  0.0231, -0.0630, -0.0390,\n",
            "        -0.0502, -0.0498,  0.0654, -0.0068,  0.0787,  0.0502,  0.0221, -0.0218,\n",
            "        -0.0744, -0.0251, -0.0095,  0.0506,  0.0706, -0.0653, -0.0840,  0.0766,\n",
            "         0.0028, -0.0592,  0.0749,  0.0430,  0.0217, -0.0637,  0.0758,  0.0799,\n",
            "         0.0014, -0.0710, -0.0708, -0.0517, -0.0593, -0.0699, -0.0067, -0.0781,\n",
            "         0.0636, -0.0648, -0.0742, -0.0682,  0.0517, -0.0253, -0.0287, -0.0734,\n",
            "         0.0189, -0.0898, -0.0087, -0.0003, -0.0542, -0.0838,  0.0689, -0.0513,\n",
            "        -0.0486, -0.0150,  0.0768,  0.0563,  0.0459, -0.0577, -0.0422, -0.0736,\n",
            "        -0.0085, -0.0909,  0.0865,  0.0315, -0.0141,  0.0436,  0.0715,  0.0288,\n",
            "         0.0166, -0.0723, -0.0619, -0.0826])\n",
            "fc2.weight_orig tensor([[ 0.0484, -0.0346, -0.0517,  ...,  0.0252,  0.0550, -0.0736],\n",
            "        [-0.0055, -0.0636, -0.0633,  ...,  0.0175, -0.0647,  0.0026],\n",
            "        [ 0.0398, -0.0347,  0.0003,  ..., -0.0510,  0.0807,  0.0636],\n",
            "        ...,\n",
            "        [-0.0706, -0.0477,  0.0034,  ..., -0.0643, -0.0822,  0.0611],\n",
            "        [ 0.0236, -0.0417,  0.0169,  ..., -0.0808,  0.0277, -0.0643],\n",
            "        [ 0.0457, -0.0455, -0.0153,  ...,  0.0733,  0.0900, -0.0228]])\n",
            "fc3.bias tensor([ 0.0955, -0.0438, -0.0262, -0.0943,  0.0384,  0.0565, -0.1031,  0.0337,\n",
            "        -0.0359, -0.0775])\n",
            "fc3.weight_orig tensor([[ 0.0611, -0.0775, -0.0561, -0.1043, -0.0382,  0.0200, -0.0030,  0.0131,\n",
            "         -0.0474,  0.0261,  0.0848,  0.0864, -0.0129,  0.0391, -0.0877, -0.0868,\n",
            "          0.0612,  0.0973, -0.0159, -0.0333, -0.0040,  0.1042,  0.0573,  0.0372,\n",
            "         -0.0816, -0.0010,  0.0268,  0.0260, -0.0117,  0.0707,  0.0958,  0.0721,\n",
            "         -0.0691, -0.0272, -0.0825, -0.0094,  0.0977, -0.0761, -0.0810, -0.0190,\n",
            "          0.0942,  0.0104, -0.0274, -0.0724, -0.0831,  0.0796,  0.0423,  0.0638,\n",
            "          0.0701,  0.0332, -0.0959,  0.0798, -0.0163, -0.0565, -0.0058,  0.0884,\n",
            "         -0.0613, -0.0738, -0.0004,  0.0005,  0.0375, -0.0181, -0.0408,  0.0552,\n",
            "          0.0134, -0.0998,  0.0267,  0.0767, -0.0102, -0.0259,  0.0336, -0.0969,\n",
            "         -0.0488,  0.0865,  0.0615, -0.0209,  0.0491,  0.1073,  0.0741,  0.0527,\n",
            "          0.0516, -0.0096,  0.0418,  0.0632],\n",
            "        [-0.0477, -0.0918, -0.0248, -0.0524, -0.0309, -0.0899, -0.0629, -0.0442,\n",
            "          0.0768,  0.0948, -0.0669, -0.0690,  0.0304, -0.0540, -0.1054,  0.0341,\n",
            "         -0.0008, -0.0327,  0.1075,  0.0245, -0.0514,  0.0570,  0.0695,  0.0955,\n",
            "         -0.0845, -0.0431, -0.0961,  0.0884, -0.0232, -0.0876, -0.0565,  0.0844,\n",
            "          0.0780,  0.0672,  0.0125,  0.0149,  0.0809, -0.0714, -0.1051, -0.0285,\n",
            "          0.0677, -0.0188,  0.0719,  0.0723, -0.0749, -0.0185,  0.0938, -0.0611,\n",
            "          0.0608,  0.0452, -0.0546,  0.0837,  0.1080,  0.0283, -0.0872,  0.0026,\n",
            "          0.0215,  0.0283,  0.0604, -0.0467,  0.0116, -0.0688, -0.0122, -0.0935,\n",
            "         -0.0619, -0.0390, -0.0054,  0.0084, -0.0428, -0.0473, -0.0239, -0.1029,\n",
            "          0.0771, -0.0220, -0.0072,  0.0710,  0.0723,  0.0851,  0.0060, -0.1084,\n",
            "         -0.0669, -0.0843,  0.0976, -0.0671],\n",
            "        [ 0.0011, -0.0744, -0.0073, -0.0912,  0.0062,  0.0415,  0.0566, -0.0686,\n",
            "         -0.0766,  0.0247, -0.0954, -0.0311,  0.0489,  0.0827,  0.0939,  0.0182,\n",
            "          0.0154,  0.0577,  0.0677,  0.0054, -0.0284,  0.0212, -0.0373,  0.0620,\n",
            "         -0.0988,  0.0638, -0.0617,  0.1028, -0.0574,  0.0573, -0.0099,  0.0655,\n",
            "          0.0351,  0.0064, -0.0645, -0.0569,  0.0325, -0.1060, -0.0664,  0.0225,\n",
            "          0.0636, -0.0180, -0.0170,  0.0978, -0.1046,  0.0721,  0.0768, -0.0370,\n",
            "          0.0538, -0.1052, -0.0462,  0.0216, -0.0380, -0.0065,  0.0855,  0.0253,\n",
            "         -0.0332,  0.0890, -0.0732,  0.0467, -0.0805, -0.0656,  0.0192,  0.0333,\n",
            "          0.1006, -0.0770, -0.0471, -0.1090, -0.0927,  0.0187,  0.0175, -0.0862,\n",
            "         -0.0331,  0.0773,  0.0522,  0.0603,  0.0878,  0.0253,  0.0598, -0.1073,\n",
            "         -0.0436,  0.0274,  0.0506, -0.1036],\n",
            "        [ 0.0290,  0.0788,  0.0572,  0.0283,  0.0135,  0.0753, -0.0638,  0.0861,\n",
            "          0.1042, -0.0248,  0.0645,  0.0187, -0.0763, -0.0993,  0.0674, -0.0174,\n",
            "          0.0608, -0.0298,  0.0836,  0.0272,  0.0390,  0.0608, -0.0937,  0.0374,\n",
            "          0.0864,  0.0670,  0.0608, -0.0696, -0.1018, -0.0556,  0.0346,  0.0464,\n",
            "         -0.0550,  0.0106, -0.0514,  0.0763, -0.0777,  0.0962,  0.0730, -0.0448,\n",
            "         -0.0072,  0.0588, -0.0724, -0.0236,  0.0074,  0.0145, -0.0061, -0.0544,\n",
            "          0.0982, -0.0440, -0.1066,  0.0156,  0.0210,  0.1046,  0.0751, -0.0724,\n",
            "          0.0555, -0.0542,  0.0477, -0.0863, -0.0140, -0.0743, -0.1064, -0.0085,\n",
            "         -0.0034, -0.0088, -0.0808, -0.0328, -0.0120,  0.0167, -0.0447, -0.0228,\n",
            "         -0.0233,  0.0662,  0.1004, -0.0584, -0.0390,  0.0471, -0.0447, -0.0965,\n",
            "         -0.0972, -0.0380, -0.0483,  0.0021],\n",
            "        [ 0.1060, -0.0167,  0.0066,  0.0396, -0.0736, -0.0550,  0.0174, -0.1015,\n",
            "          0.1083, -0.0300, -0.0138, -0.0475, -0.0960, -0.0892, -0.0076,  0.0419,\n",
            "         -0.0022, -0.0841, -0.0939,  0.0416,  0.0266, -0.0959, -0.0860,  0.0551,\n",
            "          0.1025,  0.0394, -0.0660,  0.0449,  0.0896, -0.0774, -0.0336,  0.0444,\n",
            "          0.0260, -0.0188, -0.0739, -0.0391,  0.0817, -0.0933, -0.0091, -0.0141,\n",
            "         -0.0006,  0.0901, -0.0942, -0.0621,  0.0488,  0.0247, -0.1037, -0.0150,\n",
            "          0.0023,  0.0599, -0.0946,  0.0339, -0.0162, -0.0450,  0.0928,  0.0949,\n",
            "         -0.0154,  0.1076, -0.0420, -0.0940,  0.0291, -0.0835,  0.0849, -0.0279,\n",
            "          0.0249, -0.0274,  0.0260, -0.0557, -0.0330, -0.0938,  0.1024,  0.1066,\n",
            "         -0.0952,  0.0912,  0.0262, -0.1084, -0.0241,  0.0323,  0.0671, -0.0864,\n",
            "          0.0654,  0.0888,  0.0956, -0.0043],\n",
            "        [-0.1002,  0.0118,  0.0976, -0.0590,  0.1073, -0.1083, -0.0521,  0.0123,\n",
            "         -0.0025, -0.0647,  0.0734,  0.0613,  0.0228, -0.0607, -0.1061, -0.0641,\n",
            "          0.0935, -0.0429,  0.0778, -0.0955, -0.0029,  0.1071, -0.0635, -0.0067,\n",
            "         -0.0068,  0.0228, -0.0391,  0.0757,  0.0121,  0.0976,  0.0063,  0.1078,\n",
            "         -0.0061,  0.0642,  0.0233,  0.0152, -0.0111, -0.0429,  0.0170,  0.0840,\n",
            "         -0.0050,  0.0680, -0.0413,  0.0926,  0.0609,  0.0059,  0.0107, -0.1032,\n",
            "          0.0813,  0.0991, -0.0865, -0.0845, -0.0414,  0.0251, -0.0111, -0.0255,\n",
            "          0.0557, -0.0935, -0.0712,  0.0154,  0.1083, -0.0496, -0.0033, -0.0174,\n",
            "          0.0818,  0.0264,  0.0082,  0.0410, -0.0384,  0.0557,  0.0375,  0.0966,\n",
            "          0.1089, -0.0432,  0.0968, -0.0663, -0.0475, -0.0741, -0.0242,  0.1077,\n",
            "          0.0727,  0.0561, -0.0643, -0.0064],\n",
            "        [-0.0865, -0.0723, -0.0820,  0.0249, -0.0225,  0.0970,  0.0415,  0.0643,\n",
            "         -0.0295,  0.0119, -0.0343,  0.0836,  0.0764, -0.0984,  0.0446, -0.0093,\n",
            "          0.0184, -0.0890,  0.0167,  0.1065,  0.0086, -0.0506,  0.1053, -0.0881,\n",
            "          0.0248, -0.0170, -0.0083,  0.0649,  0.0569, -0.0385, -0.0960, -0.0037,\n",
            "          0.0269, -0.0980,  0.0985,  0.0381, -0.0351,  0.0546,  0.0192, -0.0342,\n",
            "         -0.0119, -0.1007,  0.0526,  0.0791,  0.1027,  0.0930, -0.0102,  0.0699,\n",
            "         -0.0577,  0.0751, -0.0540,  0.0766,  0.0174,  0.1086,  0.0342,  0.0584,\n",
            "         -0.0658, -0.0398, -0.0448, -0.0636,  0.0901,  0.0729,  0.1043,  0.0568,\n",
            "          0.0810, -0.0218, -0.0944, -0.1059,  0.1045, -0.0734, -0.0297,  0.0586,\n",
            "         -0.0270, -0.0224,  0.0587, -0.0763,  0.0628, -0.0977,  0.0746, -0.0770,\n",
            "         -0.0510,  0.0857, -0.0248,  0.0486],\n",
            "        [-0.1022, -0.0588,  0.0284, -0.1017, -0.0005,  0.0676, -0.0925, -0.0472,\n",
            "          0.0966,  0.0458,  0.0770,  0.0301,  0.0973, -0.0527, -0.0788,  0.0287,\n",
            "          0.0284,  0.0737, -0.0105, -0.0723,  0.0667,  0.0417,  0.0406,  0.0834,\n",
            "          0.1014, -0.0140, -0.0794,  0.0153,  0.0829, -0.1015, -0.0166, -0.0065,\n",
            "          0.0152, -0.0296, -0.0234, -0.1020,  0.0500, -0.0333,  0.0485,  0.0633,\n",
            "         -0.0941, -0.0779,  0.0192, -0.1043,  0.0050,  0.0702, -0.0313, -0.0741,\n",
            "         -0.0482, -0.0562, -0.0035, -0.0834, -0.0646,  0.1045, -0.0764, -0.0645,\n",
            "         -0.0657, -0.0057,  0.0740, -0.0970, -0.0260,  0.0251, -0.0248, -0.0947,\n",
            "         -0.0079, -0.0982, -0.1078,  0.0152,  0.0107,  0.1037, -0.0192, -0.0450,\n",
            "         -0.0739, -0.0567,  0.0357,  0.0331,  0.0636, -0.0596,  0.0387, -0.0945,\n",
            "          0.0681,  0.0523,  0.0273,  0.0819],\n",
            "        [ 0.0375, -0.0833,  0.0775, -0.0769,  0.0582, -0.0195,  0.0655,  0.0194,\n",
            "         -0.0720, -0.0390,  0.0504,  0.0775,  0.0747, -0.0299,  0.0812,  0.0200,\n",
            "         -0.0700, -0.0071,  0.0953, -0.0236,  0.0675,  0.0261, -0.0694,  0.0568,\n",
            "          0.0701,  0.0142, -0.0644, -0.0918,  0.0099,  0.0855, -0.0877, -0.0826,\n",
            "          0.0671,  0.0643,  0.0523, -0.0992, -0.0492, -0.0858,  0.0620,  0.0117,\n",
            "         -0.0267,  0.0268,  0.0533, -0.0215, -0.0901,  0.0283,  0.0943, -0.0995,\n",
            "          0.0817, -0.0513,  0.0998,  0.0363, -0.0727,  0.0054, -0.0766, -0.0319,\n",
            "          0.0098,  0.0888, -0.0475, -0.0648,  0.0173,  0.0445, -0.0564, -0.0506,\n",
            "          0.0702, -0.0284,  0.1008, -0.0343, -0.0395, -0.0230,  0.0070,  0.0809,\n",
            "         -0.0498,  0.0867, -0.0757, -0.0976, -0.0968, -0.0082,  0.0207, -0.0008,\n",
            "          0.0127, -0.1021, -0.0291, -0.0009],\n",
            "        [-0.0296, -0.0205, -0.0605, -0.0822,  0.0312,  0.0960,  0.0472, -0.1013,\n",
            "          0.0372,  0.0144,  0.0272,  0.0354,  0.0752, -0.0032, -0.0881,  0.0374,\n",
            "         -0.1048,  0.0167,  0.0437, -0.0428,  0.0656, -0.0221,  0.0467, -0.0744,\n",
            "         -0.0420, -0.0003, -0.0579,  0.0071,  0.1062,  0.0162, -0.0823, -0.1036,\n",
            "         -0.0011,  0.0306,  0.0232, -0.0730, -0.0974,  0.0322,  0.0646, -0.0853,\n",
            "         -0.0087, -0.1084, -0.0702,  0.0116, -0.0386,  0.0269,  0.0954,  0.0021,\n",
            "         -0.1024, -0.0904,  0.0353, -0.0801, -0.0663,  0.0997,  0.0383,  0.0637,\n",
            "          0.0374,  0.0635, -0.0257, -0.0886,  0.0780, -0.0698,  0.0965, -0.0326,\n",
            "          0.0324, -0.0060,  0.1041, -0.0803, -0.0144,  0.0542,  0.0603, -0.0640,\n",
            "          0.0067, -0.0756, -0.0348, -0.0944,  0.0996,  0.1027,  0.0810,  0.0255,\n",
            "         -0.0894, -0.0624,  0.1091,  0.0032]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5yZJ05tTrLU",
        "colab_type": "text"
      },
      "source": [
        "Now we can check the sparsity induced in every pruned parameter, which **will not be equal to 20% in each layer**. However, the **global sparsity will be (approximately) 20%**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNGVLP61S47H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d04df518-49f4-4b29-ebc6-b3ae5aa2837c"
      },
      "source": [
        "# Print the Sparsity per Layer for the Pruned Model\n",
        "# The total Sparsity of the Model will be around 20%\n",
        "\n",
        "# Sparsity % Calculation per Layer = \n",
        "# 100 * (Number of zero weights after pruning in the layer) / (Total Number of Weights in the layer)\n",
        "\n",
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc3.weight == 0))\n",
        "        / float(model.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "\n",
        "# Print Global Sparsity of the Model across all layers\n",
        "\n",
        "# Sparsity % Calculation for the Model = \n",
        "# 100 * sum(Number of zero weights after pruning in each layer of the model layer) / sum(Total Number of Weights in each layer)\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sparsity in conv1.weight: 7.41%\n",
            "Sparsity in conv2.weight: 9.03%\n",
            "Sparsity in fc1.weight: 21.98%\n",
            "Sparsity in fc2.weight: 12.46%\n",
            "Sparsity in fc3.weight: 9.76%\n",
            "Global sparsity: 20.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRhcd5KWbJjS",
        "colab_type": "text"
      },
      "source": [
        "# Extending \"torch.nn.utils.prune\" Functionality with Custom Pruning Functions\n",
        "\n",
        "## Steps for creating Custom Pruning Functions\n",
        "\n",
        "1. To implement your own pruning function, you can extend the **nn.utils.prune module** by subclassing the **BasePruningMethod** base class, the same way all other pruning methods do. \n",
        "\n",
        "2. The base class implements the following methods for you: \n",
        "  \n",
        "  **_ __call_ _ _**\n",
        "\n",
        "  **apply_mask**\n",
        "\n",
        "  **apply**\n",
        "\n",
        "  **prune**\n",
        "\n",
        "  **remove**\n",
        "\n",
        "  Beyond some special cases, you shouldn’t have to reimplement these methods for your new pruning technique. \n",
        "  \n",
        "3. You will, however, have to implement __init__ (the constructor), and compute_mask (the instructions on how to compute the mask for the given tensor according to the logic of your pruning technique). \n",
        "\n",
        "4. In addition, you will have to specify which type of pruning this technique implements (supported options are **global, structured, and unstructured**). This is needed to determine **how to combine masks in the case in which pruning is applied** iteratively. In other words, when pruning a pre-pruned parameter, the current prunining techique is expected to act on the unpruned portion of the parameter. Specifying the **PRUNING_TYPE** will enable the **PruningContainer** (which handles the iterative application of pruning masks) to correctly identify the slice of the parameter to prune.\n",
        "\n",
        "Let’s assume, for example, that you want to implement a pruning technique that **prunes every other entry in a tensor** (or – if the tensor has previously been pruned – in the remaining unpruned portion of the tensor). This will be of **PRUNING_TYPE='unstructured'** because it acts on individual connections in a layer and not on entire **units/channels ('structured')**, or **across different parameters ('global')**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d61_6mCVoj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Firstly, create the Custom Pruning Class with the Method/Function\n",
        "# This custom Class Subclasses the \"BasePruningMethod\".\n",
        "class customPruningMethod(prune.BasePruningMethod):\n",
        "  # Define the PRUNING TYPE i.e. structured, unstructured or global\n",
        "  PRUNING_TYPE = 'unstructured'\n",
        "\n",
        "  # Define the function for Custom Pruning\n",
        "  # Here we are defining a function that prunes every alternate value in the tensor\n",
        "  # t: input tensor, default_mask: 'weight' or 'bias'\n",
        "  def compute_mask(self, t, default_mask):\n",
        "    # Crate a Copy of the 'default_mask' with same size/shape as the tensor\n",
        "    mask = default_mask.clone()\n",
        "    # Create a Mask with Every Alternate Values in the 'mask' equals to '0'\n",
        "    # [::2] => all rows, columns and hop of 2\n",
        "    mask.view(-1)[::2] = 0\n",
        "    return mask"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RHo4Fu0f8wm",
        "colab_type": "text"
      },
      "source": [
        "Now, to apply this to a parameter in an nn.Module, you should also provide **a simple function that instantiates the method and applies it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLKwuxcrf5w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Function to call the Custom Pruning function from the Class\n",
        "def custom_unstructured(module, name):\n",
        "  \"\"\"\n",
        "  Prunes tensor corresponding to parameter called `name` in `module`\n",
        "    by removing every other entry in the tensors.\n",
        "    Modifies module in place (and also return the modified module)\n",
        "    by:\n",
        "    1) adding a named buffer called `name+'_mask'` corresponding to the\n",
        "    binary mask applied to the parameter `name` by the pruning method.\n",
        "    The parameter `name` is replaced by its pruned version, while the\n",
        "    original (unpruned) parameter is stored in a new parameter named\n",
        "    `name+'_orig'`.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): module containing the tensor to prune\n",
        "        name (string): parameter name within `module` on which pruning\n",
        "                will act.\n",
        "\n",
        "    Returns:\n",
        "        module (nn.Module): modified (i.e. pruned) version of the input\n",
        "            module\n",
        "\n",
        "    Examples:\n",
        "        >>> m = nn.Linear(3, 4)\n",
        "        >>> foobar_unstructured(m, name='bias')\n",
        "  \"\"\"\n",
        "  customPruningMethod.apply(module, name)\n",
        "  return module"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwxhP3LehJtQ",
        "colab_type": "text"
      },
      "source": [
        "## Try out the Custom Pruning Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIzYJlK4hJcX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d9caf448-960c-4d51-aca3-b52c5cdfcb5c"
      },
      "source": [
        "# Instantiate the Model\n",
        "model = LeNet()\n",
        "model"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LeNet(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08-pZT3lglWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b04045b-a5c9-4360-b092-8626c928de15"
      },
      "source": [
        "# Original Model FC3 Layer Weights\n",
        "model.fc3.weight.data"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.1730e-02, -1.0493e-01,  3.7107e-02, -8.1432e-02,  3.6493e-03,\n",
              "         -1.6143e-02, -5.9364e-02, -7.9780e-02,  4.9360e-03,  2.9754e-02,\n",
              "         -1.0489e-01,  4.8641e-02, -2.4668e-02, -2.8385e-02, -7.2158e-02,\n",
              "         -7.9618e-02,  6.7679e-02,  7.3266e-02,  1.2629e-03,  4.1988e-02,\n",
              "          8.9891e-02, -9.7443e-02, -7.8273e-02,  3.7919e-02, -2.4429e-02,\n",
              "          5.4867e-02,  9.6588e-02,  1.0614e-01, -9.7069e-02, -9.4999e-02,\n",
              "          9.5408e-02,  5.3908e-02, -8.8701e-02,  1.7467e-04,  1.9297e-02,\n",
              "          1.0780e-01, -2.4014e-02,  2.8175e-02,  3.4580e-02,  7.2127e-03,\n",
              "         -9.9485e-02, -6.3003e-03,  7.2915e-02, -1.3120e-02,  8.7878e-02,\n",
              "          3.6286e-02,  9.8314e-03, -1.0587e-01, -4.7081e-02, -5.5500e-03,\n",
              "          3.6411e-02,  5.2733e-02, -3.4934e-02, -7.0024e-02, -1.0029e-01,\n",
              "         -2.9627e-04,  5.3550e-02, -2.2353e-02, -8.7671e-02,  2.2571e-02,\n",
              "         -8.5857e-02, -6.5263e-02,  1.6634e-02,  6.1099e-03,  2.5267e-02,\n",
              "          9.3630e-02,  7.3828e-02, -8.0271e-02,  1.0233e-01, -4.3495e-02,\n",
              "          2.9665e-02,  1.2554e-02,  2.2214e-02,  8.2521e-02, -7.5031e-02,\n",
              "         -2.2133e-02, -1.1640e-02, -7.0648e-03,  6.5373e-02, -2.0827e-02,\n",
              "         -8.7598e-02,  2.9617e-02,  8.9420e-02, -5.0330e-02],\n",
              "        [ 6.1765e-02, -3.2742e-02, -3.7806e-02,  3.7579e-02,  6.2637e-02,\n",
              "         -5.0562e-02,  9.8020e-03,  1.8424e-02,  3.4951e-02, -1.6185e-02,\n",
              "         -7.6546e-02, -3.1221e-02,  8.9817e-02,  2.8728e-02,  5.4052e-02,\n",
              "         -9.5449e-02, -5.8493e-02,  7.1536e-02,  5.1969e-03, -1.6864e-02,\n",
              "          6.9522e-02,  1.6782e-02, -4.8411e-02,  3.1076e-02,  7.1230e-02,\n",
              "         -3.1873e-02,  3.3594e-02, -1.3989e-02,  1.0340e-01,  7.5418e-02,\n",
              "          9.1857e-02, -1.0622e-01, -6.4296e-02,  5.0834e-02,  2.6732e-02,\n",
              "         -1.0125e-02,  8.1524e-05,  2.0837e-02,  6.0345e-02,  6.9842e-02,\n",
              "          4.1504e-02, -4.5014e-03, -8.5011e-02, -1.0383e-01,  7.9232e-02,\n",
              "         -9.8986e-02, -1.1496e-02,  8.9284e-02,  9.9734e-02,  1.7257e-02,\n",
              "         -3.4697e-02, -7.0025e-02, -8.5201e-02, -7.9439e-02,  9.3519e-02,\n",
              "          8.5714e-02, -6.9826e-02,  8.2259e-02,  9.6352e-03, -6.2499e-02,\n",
              "          1.5664e-02, -3.8537e-02,  4.1651e-03, -6.9998e-02,  7.5516e-02,\n",
              "          5.6416e-02,  1.5223e-02,  5.5527e-02,  3.5096e-02,  6.6490e-02,\n",
              "         -2.0533e-02, -7.7883e-03,  8.0517e-02, -9.9689e-02,  8.7230e-02,\n",
              "         -2.1810e-02,  1.0625e-01, -8.3467e-02, -1.7063e-02,  8.0401e-02,\n",
              "         -5.7308e-02, -1.0116e-01, -4.5644e-02,  6.2046e-02],\n",
              "        [-3.0207e-02, -4.9265e-02, -7.8169e-02, -4.7056e-02, -1.0296e-01,\n",
              "          7.7934e-02, -6.8106e-02, -1.0165e-01, -6.6936e-02, -6.8023e-02,\n",
              "         -1.9384e-02, -3.8030e-02,  4.4413e-02, -5.9872e-02, -4.9903e-02,\n",
              "         -1.0815e-01, -2.4736e-02, -9.0996e-02, -1.4870e-02,  1.9921e-03,\n",
              "         -9.3310e-02, -5.5095e-02, -4.9532e-02, -2.0856e-02, -2.4203e-02,\n",
              "         -4.4474e-02, -9.2474e-02, -3.6562e-03,  3.7452e-02, -2.7131e-02,\n",
              "          4.6712e-02, -6.6078e-03, -7.1629e-02, -3.7679e-02,  9.2900e-02,\n",
              "          1.4964e-02, -1.5378e-02, -8.3813e-03, -1.7363e-02, -1.0366e-01,\n",
              "         -6.2700e-02,  1.4958e-02,  5.0562e-02,  9.5583e-02, -2.6530e-02,\n",
              "         -5.0009e-02,  2.3473e-02,  5.6736e-02, -7.7336e-02, -9.1291e-02,\n",
              "          6.9679e-02, -2.2945e-02, -4.5333e-02, -3.2717e-02, -9.3977e-02,\n",
              "         -9.6759e-02, -4.8498e-02,  9.4575e-02,  5.7546e-02,  9.2507e-02,\n",
              "          3.9342e-02,  2.5881e-02,  3.6951e-03,  3.6245e-02, -9.8106e-02,\n",
              "         -3.5269e-02, -3.1215e-02,  5.3546e-02, -6.1060e-02, -9.5622e-02,\n",
              "          3.3774e-02,  3.9724e-02,  4.9821e-03,  9.2694e-02,  8.7729e-03,\n",
              "         -7.0413e-02, -2.0649e-02,  2.9078e-02,  7.9589e-02, -4.4273e-02,\n",
              "         -2.4590e-02,  5.4786e-03,  1.4469e-02, -1.0254e-01],\n",
              "        [ 3.8400e-02,  6.2173e-02,  3.1355e-02,  3.8366e-02, -6.6269e-02,\n",
              "          3.3050e-02,  2.5437e-03, -3.6605e-05,  5.5945e-03,  1.5299e-02,\n",
              "         -4.8703e-02, -2.3772e-02,  4.3968e-03,  1.9618e-02,  4.5889e-02,\n",
              "          1.0027e-01,  1.0458e-01, -1.0636e-01,  5.5792e-02, -3.0275e-02,\n",
              "          5.1188e-02, -9.1530e-03,  3.4569e-02, -2.9604e-03, -7.5677e-02,\n",
              "          3.3696e-02, -9.4844e-02,  1.0219e-01,  2.4205e-02,  1.4120e-03,\n",
              "         -5.3056e-02,  9.8865e-02, -1.5425e-02,  1.0507e-01, -1.4355e-02,\n",
              "         -6.0813e-02, -3.7323e-02, -6.1574e-02, -4.2707e-03, -4.8495e-02,\n",
              "          1.0337e-01, -4.2213e-02,  9.7399e-02, -4.3836e-02,  1.0243e-01,\n",
              "         -1.0009e-01, -7.1944e-02,  7.4439e-02, -1.0038e-01, -1.0327e-01,\n",
              "          5.3161e-02,  6.2355e-02, -5.0352e-02,  8.1351e-02,  2.2432e-03,\n",
              "          4.6447e-02,  1.0247e-01,  3.8232e-02,  8.9837e-02, -9.3185e-02,\n",
              "         -1.0325e-01,  8.8519e-02, -5.1487e-02, -6.4115e-02, -1.5130e-02,\n",
              "          1.8804e-02,  4.0074e-02,  3.5409e-02,  1.1358e-02,  4.0965e-02,\n",
              "         -5.9482e-02, -1.2862e-02,  8.7251e-03,  9.1661e-03, -1.1262e-02,\n",
              "          2.0882e-02,  1.6403e-02,  6.1808e-02,  3.2485e-02,  1.0800e-02,\n",
              "         -5.0873e-02, -3.9295e-02, -4.5332e-02,  4.1433e-02],\n",
              "        [ 5.2487e-02,  1.0302e-01, -9.4899e-03, -5.7540e-02, -6.2065e-02,\n",
              "          5.5409e-02, -7.3627e-02,  2.1482e-02,  5.3548e-02, -7.0117e-02,\n",
              "         -1.0423e-01,  5.9352e-02, -6.7813e-02,  6.4901e-02, -8.6976e-02,\n",
              "         -3.8182e-02, -8.4110e-02, -9.8763e-02,  3.4984e-02, -2.7801e-02,\n",
              "         -1.3223e-02,  3.8846e-02, -2.4761e-03, -9.7178e-02, -7.4123e-03,\n",
              "          7.5921e-02, -8.5968e-02, -8.2125e-02,  1.9591e-02, -9.5969e-02,\n",
              "          9.2433e-02, -9.3333e-02,  1.0265e-01,  9.6986e-02, -7.9519e-02,\n",
              "         -5.3160e-02, -9.4352e-02, -5.4027e-02, -6.7571e-02, -9.8310e-02,\n",
              "          8.2958e-02, -3.0369e-02,  1.0776e-01,  2.6109e-02, -5.1419e-02,\n",
              "          7.5431e-03, -1.0662e-02, -1.1603e-02, -4.4774e-02, -8.8605e-02,\n",
              "         -7.0945e-02,  7.1507e-02, -4.9618e-03,  8.3952e-02,  5.1604e-02,\n",
              "         -8.4641e-02,  1.8028e-02,  5.3431e-02, -6.8960e-02, -1.9060e-02,\n",
              "          3.5826e-02, -3.0236e-02, -5.9391e-03,  8.2250e-02,  5.6108e-02,\n",
              "         -1.8509e-02,  5.2953e-02, -1.0838e-01, -3.6391e-02, -9.4325e-02,\n",
              "         -4.9503e-02,  3.1227e-03,  1.0477e-01,  7.8214e-02,  5.8993e-02,\n",
              "          9.8550e-02, -1.0238e-01,  8.9369e-03,  4.8475e-03, -4.3235e-04,\n",
              "          2.5944e-02,  9.4394e-02, -4.8513e-02, -4.3038e-02],\n",
              "        [ 3.0825e-02,  6.2409e-02,  6.0974e-02,  3.6670e-02,  9.1238e-02,\n",
              "         -2.6338e-02,  8.2921e-02,  4.0068e-02,  8.6235e-02, -6.0851e-02,\n",
              "          8.1926e-02, -1.8589e-02,  3.0650e-02, -2.5290e-02, -9.7945e-02,\n",
              "          4.6419e-02, -9.3440e-02, -3.4343e-02,  8.1332e-03,  4.1897e-02,\n",
              "         -4.0132e-02, -7.5920e-02, -8.5887e-02,  5.8399e-02, -6.0608e-02,\n",
              "          9.3923e-02, -2.6352e-02,  4.3955e-02, -1.6584e-02,  8.2295e-02,\n",
              "         -2.1314e-02, -9.6882e-02,  2.4418e-02,  1.9152e-02,  6.7152e-02,\n",
              "         -5.1149e-02,  8.2486e-02, -3.1765e-02,  1.8322e-02,  1.1868e-02,\n",
              "          2.1712e-02, -3.0377e-02,  9.4706e-02, -8.3867e-04, -4.1186e-02,\n",
              "          4.2159e-03, -6.8427e-02,  8.2436e-02,  7.6090e-02, -6.3310e-02,\n",
              "         -6.3325e-02,  8.5819e-02, -6.2294e-02, -2.5278e-02, -1.0561e-01,\n",
              "          4.1492e-02,  2.0509e-02,  6.9674e-03,  3.2608e-02,  4.1610e-02,\n",
              "         -4.2967e-02, -1.0464e-01, -2.8344e-02, -1.0742e-01, -7.2514e-02,\n",
              "          1.5445e-02,  5.6294e-02, -6.8488e-02, -6.2355e-02,  1.0181e-01,\n",
              "          4.8648e-02,  9.1330e-02, -7.2170e-02, -8.5273e-02, -4.5945e-04,\n",
              "          6.1599e-02,  1.5898e-03,  5.5886e-02, -4.0475e-02, -3.5475e-03,\n",
              "          2.5197e-02, -1.6915e-02,  4.9358e-02,  1.0827e-02],\n",
              "        [-9.7347e-02,  3.4828e-02,  9.9064e-02, -8.4951e-03,  9.0131e-02,\n",
              "         -5.8996e-02,  5.6027e-02,  4.7986e-02,  3.2951e-02,  7.0320e-02,\n",
              "         -9.7756e-02, -8.5354e-02, -5.6996e-02,  4.8346e-02, -4.5842e-02,\n",
              "         -1.9250e-02,  1.0865e-01,  7.1677e-03,  3.4506e-03, -1.0589e-01,\n",
              "         -1.0702e-02,  3.3991e-02,  7.2910e-02, -1.0520e-01,  1.2450e-02,\n",
              "         -3.1050e-02,  1.6925e-03, -1.9149e-02,  5.0827e-02, -4.1264e-03,\n",
              "         -5.9350e-02,  1.0384e-01, -6.9598e-02, -5.4403e-02,  8.3574e-03,\n",
              "         -2.3477e-02,  9.4389e-02, -2.8335e-02, -3.3704e-02, -6.1378e-03,\n",
              "          2.6858e-02, -9.6799e-02,  7.3090e-02,  2.1652e-02, -7.5839e-02,\n",
              "          8.2248e-02, -6.8438e-04, -9.6615e-02,  2.2657e-03, -5.5315e-02,\n",
              "          3.6338e-02, -2.2182e-02, -2.8801e-02, -7.9294e-02, -5.4749e-02,\n",
              "          5.1022e-02, -1.0859e-01,  1.0452e-01,  7.8722e-02, -2.7248e-02,\n",
              "          1.7391e-02,  1.0752e-01, -6.1425e-02, -4.8364e-02, -5.5888e-02,\n",
              "          8.8868e-02, -4.5883e-02,  7.6912e-02, -6.1779e-02,  8.5017e-03,\n",
              "          5.2116e-02, -5.5666e-02, -3.4105e-02,  4.2132e-02, -5.7031e-02,\n",
              "         -8.8212e-02, -8.9350e-02, -3.7039e-02, -1.0151e-01, -3.4223e-02,\n",
              "         -6.5036e-02,  3.8042e-02,  6.7237e-02, -4.9177e-02],\n",
              "        [-9.3452e-02, -7.7191e-02, -5.5627e-02, -9.1678e-02,  9.6932e-02,\n",
              "          7.0731e-03,  7.9203e-02, -8.0499e-02,  5.5115e-02,  1.5751e-02,\n",
              "         -6.4874e-02, -4.2241e-02,  1.6444e-02, -7.3635e-02, -9.3052e-02,\n",
              "         -5.2052e-02, -8.9936e-02,  3.1359e-02, -4.5374e-02,  8.1564e-02,\n",
              "         -7.1034e-02, -5.7757e-02, -1.9035e-02, -2.0427e-02, -2.5221e-02,\n",
              "          5.0697e-02,  8.2012e-02, -1.5977e-03,  2.3068e-02, -4.7000e-02,\n",
              "         -1.0906e-01,  1.0182e-01, -3.3646e-02, -2.3854e-02, -8.7005e-02,\n",
              "          7.9393e-02,  3.0552e-02,  8.8200e-02,  4.0751e-02,  1.4012e-02,\n",
              "         -7.5462e-02,  2.2494e-02,  2.9196e-02,  4.4903e-02,  1.0799e-01,\n",
              "          8.6288e-03, -2.0261e-02,  9.2253e-02,  4.4134e-02,  1.0356e-01,\n",
              "          1.1355e-02,  3.6818e-03,  9.8980e-02,  6.2554e-03, -1.5658e-02,\n",
              "         -3.2439e-02,  5.1701e-02, -3.3837e-02,  7.0204e-03, -5.6286e-02,\n",
              "         -7.1184e-02,  4.9832e-02, -2.8008e-02, -1.6175e-02, -1.0336e-01,\n",
              "          4.8702e-02, -2.0221e-03, -4.7061e-02,  9.7235e-02,  5.3388e-03,\n",
              "         -5.5813e-02, -1.5420e-03, -3.1542e-02, -4.1707e-02, -8.2693e-02,\n",
              "         -9.4787e-02,  5.9985e-02,  1.0518e-01, -2.1814e-03, -8.4722e-02,\n",
              "          2.2903e-02, -6.6022e-02,  1.0982e-02,  6.7499e-03],\n",
              "        [-1.1115e-02,  6.3300e-02,  8.7013e-02,  1.0365e-01,  1.9358e-03,\n",
              "         -1.4156e-02,  3.6088e-02,  3.8001e-02,  2.6078e-02,  1.0123e-01,\n",
              "          2.0313e-02,  2.8065e-03, -6.2678e-02,  1.0039e-02,  2.9842e-02,\n",
              "         -1.2590e-02, -4.8953e-02, -9.8177e-03, -4.9233e-02, -1.9408e-02,\n",
              "         -4.7774e-03,  8.9549e-02,  1.0457e-02,  3.9146e-02,  5.2301e-02,\n",
              "         -6.3051e-04,  5.6935e-02,  3.8543e-02, -4.5776e-02,  1.0267e-02,\n",
              "          1.0123e-02,  8.7663e-02,  9.3402e-02,  8.0215e-02, -1.6501e-02,\n",
              "         -7.1104e-02, -9.9038e-02,  2.8094e-02, -7.0248e-02,  3.9441e-02,\n",
              "         -3.2184e-02, -7.3027e-02,  4.8363e-02, -4.1713e-02,  4.6202e-02,\n",
              "          6.0016e-02,  3.0013e-03, -1.0730e-01,  1.7097e-02, -8.1422e-02,\n",
              "          7.0940e-02,  9.5075e-02,  9.2363e-02,  8.4007e-03,  1.0634e-01,\n",
              "         -1.0597e-01,  4.5429e-02,  8.4414e-02, -1.0697e-01,  8.3155e-02,\n",
              "          3.7531e-02,  1.0113e-01,  9.9451e-02,  2.2678e-02, -1.0145e-01,\n",
              "          3.5396e-02, -1.0488e-01,  2.0896e-02, -7.8262e-02,  2.5016e-02,\n",
              "          7.3192e-02, -5.2621e-02,  6.6523e-02, -9.3840e-02, -1.3478e-02,\n",
              "          1.0812e-01, -7.3118e-03, -5.0880e-02, -3.0459e-02, -6.6059e-02,\n",
              "         -4.2171e-02,  2.4520e-02, -4.4911e-02, -8.7021e-02],\n",
              "        [-3.4794e-02, -7.7001e-02, -6.8113e-02, -1.3155e-02, -7.5976e-02,\n",
              "         -3.8307e-02, -4.4908e-02,  6.6710e-02,  2.6707e-02, -3.7515e-02,\n",
              "          3.1935e-02,  7.4773e-02,  3.3477e-02,  8.0850e-04,  1.8615e-02,\n",
              "         -3.2473e-02, -1.3938e-02,  1.9698e-02, -7.6723e-02,  9.1602e-02,\n",
              "         -1.6756e-02,  3.8039e-02,  7.7992e-02,  7.9414e-02,  7.0487e-02,\n",
              "         -4.3360e-02, -6.7624e-03,  9.5167e-03,  6.5421e-02, -1.1167e-02,\n",
              "          9.5406e-02,  7.4556e-02,  4.0994e-02,  4.5604e-04,  7.5557e-02,\n",
              "          6.2758e-02,  4.8543e-02,  2.2582e-02,  7.5272e-02,  6.7874e-02,\n",
              "          8.8141e-02,  6.6061e-02, -9.9144e-02,  5.9170e-02, -7.7423e-02,\n",
              "         -2.2581e-02, -8.1304e-03, -8.4206e-02,  1.5095e-02,  5.3647e-02,\n",
              "          9.6442e-02,  4.2010e-02,  7.2335e-02,  4.4204e-03,  9.4194e-02,\n",
              "          3.5799e-02,  6.3976e-02, -6.5497e-02,  1.1973e-02,  4.4441e-02,\n",
              "          3.9483e-02, -1.1755e-02,  4.8461e-02, -1.0734e-01, -4.7172e-03,\n",
              "          1.0860e-02,  1.0456e-01,  5.3121e-02,  7.2797e-02, -6.0271e-02,\n",
              "          6.0316e-02, -4.6270e-02,  5.2793e-02,  6.4519e-02, -9.9277e-02,\n",
              "         -2.4558e-02,  5.3869e-02,  9.8981e-02,  1.6658e-02, -1.0113e-01,\n",
              "         -8.9723e-02,  5.9160e-02, -1.0033e-01,  5.9781e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_IzAfhJhZb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de771ca0-7fbb-443b-a906-a0811d9539b2"
      },
      "source": [
        "# Shape of Original FC3 Layer Weight data\n",
        "model.fc3.weight.data.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 84])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaGgaoW2huPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3411d252-c8bf-40e0-ed52-6adc688953e2"
      },
      "source": [
        "# Original Model FC3 Layer Bias Data\n",
        "model.fc3.bias.data"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0654, -0.0262, -0.0466, -0.0926, -0.0189,  0.0428,  0.0982, -0.0424,\n",
              "         0.0266,  0.0498])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGe3IIlvh23P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a88d7533-cd6d-446e-ff82-ba1d59df9866"
      },
      "source": [
        "# Shape of Original FC3 Layer Bias data\n",
        "model.fc3.bias.data.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbVVUPMRiDSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "738ada95-9cd1-4104-b68a-4c97bd0633fe"
      },
      "source": [
        "u = model.fc3.bias.data\n",
        "u.view(-1)[::2] = 0\n",
        "u"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000, -0.0262,  0.0000, -0.0926,  0.0000,  0.0428,  0.0000, -0.0424,\n",
              "         0.0000,  0.0498])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpF-LP9Ehf41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d4f5c10-b4d4-49e4-d22e-db2f52ca04d7"
      },
      "source": [
        "# Apply Custom Pruning Function\n",
        "custom_unstructured(module=model.fc3, name='bias')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=84, out_features=10, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr3wiN7jiZLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e870e4b3-888b-48e7-8f0d-c2213691d350"
      },
      "source": [
        "# Module Original Named Parameters\n",
        "# Since the Pruning Mask has been applied, the 'bias' changes to 'bias_orig'\n",
        "print(list(model.fc3.named_parameters()))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight', Parameter containing:\n",
            "tensor([[ 5.1730e-02, -1.0493e-01,  3.7107e-02, -8.1432e-02,  3.6493e-03,\n",
            "         -1.6143e-02, -5.9364e-02, -7.9780e-02,  4.9360e-03,  2.9754e-02,\n",
            "         -1.0489e-01,  4.8641e-02, -2.4668e-02, -2.8385e-02, -7.2158e-02,\n",
            "         -7.9618e-02,  6.7679e-02,  7.3266e-02,  1.2629e-03,  4.1988e-02,\n",
            "          8.9891e-02, -9.7443e-02, -7.8273e-02,  3.7919e-02, -2.4429e-02,\n",
            "          5.4867e-02,  9.6588e-02,  1.0614e-01, -9.7069e-02, -9.4999e-02,\n",
            "          9.5408e-02,  5.3908e-02, -8.8701e-02,  1.7467e-04,  1.9297e-02,\n",
            "          1.0780e-01, -2.4014e-02,  2.8175e-02,  3.4580e-02,  7.2127e-03,\n",
            "         -9.9485e-02, -6.3003e-03,  7.2915e-02, -1.3120e-02,  8.7878e-02,\n",
            "          3.6286e-02,  9.8314e-03, -1.0587e-01, -4.7081e-02, -5.5500e-03,\n",
            "          3.6411e-02,  5.2733e-02, -3.4934e-02, -7.0024e-02, -1.0029e-01,\n",
            "         -2.9627e-04,  5.3550e-02, -2.2353e-02, -8.7671e-02,  2.2571e-02,\n",
            "         -8.5857e-02, -6.5263e-02,  1.6634e-02,  6.1099e-03,  2.5267e-02,\n",
            "          9.3630e-02,  7.3828e-02, -8.0271e-02,  1.0233e-01, -4.3495e-02,\n",
            "          2.9665e-02,  1.2554e-02,  2.2214e-02,  8.2521e-02, -7.5031e-02,\n",
            "         -2.2133e-02, -1.1640e-02, -7.0648e-03,  6.5373e-02, -2.0827e-02,\n",
            "         -8.7598e-02,  2.9617e-02,  8.9420e-02, -5.0330e-02],\n",
            "        [ 6.1765e-02, -3.2742e-02, -3.7806e-02,  3.7579e-02,  6.2637e-02,\n",
            "         -5.0562e-02,  9.8020e-03,  1.8424e-02,  3.4951e-02, -1.6185e-02,\n",
            "         -7.6546e-02, -3.1221e-02,  8.9817e-02,  2.8728e-02,  5.4052e-02,\n",
            "         -9.5449e-02, -5.8493e-02,  7.1536e-02,  5.1969e-03, -1.6864e-02,\n",
            "          6.9522e-02,  1.6782e-02, -4.8411e-02,  3.1076e-02,  7.1230e-02,\n",
            "         -3.1873e-02,  3.3594e-02, -1.3989e-02,  1.0340e-01,  7.5418e-02,\n",
            "          9.1857e-02, -1.0622e-01, -6.4296e-02,  5.0834e-02,  2.6732e-02,\n",
            "         -1.0125e-02,  8.1524e-05,  2.0837e-02,  6.0345e-02,  6.9842e-02,\n",
            "          4.1504e-02, -4.5014e-03, -8.5011e-02, -1.0383e-01,  7.9232e-02,\n",
            "         -9.8986e-02, -1.1496e-02,  8.9284e-02,  9.9734e-02,  1.7257e-02,\n",
            "         -3.4697e-02, -7.0025e-02, -8.5201e-02, -7.9439e-02,  9.3519e-02,\n",
            "          8.5714e-02, -6.9826e-02,  8.2259e-02,  9.6352e-03, -6.2499e-02,\n",
            "          1.5664e-02, -3.8537e-02,  4.1651e-03, -6.9998e-02,  7.5516e-02,\n",
            "          5.6416e-02,  1.5223e-02,  5.5527e-02,  3.5096e-02,  6.6490e-02,\n",
            "         -2.0533e-02, -7.7883e-03,  8.0517e-02, -9.9689e-02,  8.7230e-02,\n",
            "         -2.1810e-02,  1.0625e-01, -8.3467e-02, -1.7063e-02,  8.0401e-02,\n",
            "         -5.7308e-02, -1.0116e-01, -4.5644e-02,  6.2046e-02],\n",
            "        [-3.0207e-02, -4.9265e-02, -7.8169e-02, -4.7056e-02, -1.0296e-01,\n",
            "          7.7934e-02, -6.8106e-02, -1.0165e-01, -6.6936e-02, -6.8023e-02,\n",
            "         -1.9384e-02, -3.8030e-02,  4.4413e-02, -5.9872e-02, -4.9903e-02,\n",
            "         -1.0815e-01, -2.4736e-02, -9.0996e-02, -1.4870e-02,  1.9921e-03,\n",
            "         -9.3310e-02, -5.5095e-02, -4.9532e-02, -2.0856e-02, -2.4203e-02,\n",
            "         -4.4474e-02, -9.2474e-02, -3.6562e-03,  3.7452e-02, -2.7131e-02,\n",
            "          4.6712e-02, -6.6078e-03, -7.1629e-02, -3.7679e-02,  9.2900e-02,\n",
            "          1.4964e-02, -1.5378e-02, -8.3813e-03, -1.7363e-02, -1.0366e-01,\n",
            "         -6.2700e-02,  1.4958e-02,  5.0562e-02,  9.5583e-02, -2.6530e-02,\n",
            "         -5.0009e-02,  2.3473e-02,  5.6736e-02, -7.7336e-02, -9.1291e-02,\n",
            "          6.9679e-02, -2.2945e-02, -4.5333e-02, -3.2717e-02, -9.3977e-02,\n",
            "         -9.6759e-02, -4.8498e-02,  9.4575e-02,  5.7546e-02,  9.2507e-02,\n",
            "          3.9342e-02,  2.5881e-02,  3.6951e-03,  3.6245e-02, -9.8106e-02,\n",
            "         -3.5269e-02, -3.1215e-02,  5.3546e-02, -6.1060e-02, -9.5622e-02,\n",
            "          3.3774e-02,  3.9724e-02,  4.9821e-03,  9.2694e-02,  8.7729e-03,\n",
            "         -7.0413e-02, -2.0649e-02,  2.9078e-02,  7.9589e-02, -4.4273e-02,\n",
            "         -2.4590e-02,  5.4786e-03,  1.4469e-02, -1.0254e-01],\n",
            "        [ 3.8400e-02,  6.2173e-02,  3.1355e-02,  3.8366e-02, -6.6269e-02,\n",
            "          3.3050e-02,  2.5437e-03, -3.6605e-05,  5.5945e-03,  1.5299e-02,\n",
            "         -4.8703e-02, -2.3772e-02,  4.3968e-03,  1.9618e-02,  4.5889e-02,\n",
            "          1.0027e-01,  1.0458e-01, -1.0636e-01,  5.5792e-02, -3.0275e-02,\n",
            "          5.1188e-02, -9.1530e-03,  3.4569e-02, -2.9604e-03, -7.5677e-02,\n",
            "          3.3696e-02, -9.4844e-02,  1.0219e-01,  2.4205e-02,  1.4120e-03,\n",
            "         -5.3056e-02,  9.8865e-02, -1.5425e-02,  1.0507e-01, -1.4355e-02,\n",
            "         -6.0813e-02, -3.7323e-02, -6.1574e-02, -4.2707e-03, -4.8495e-02,\n",
            "          1.0337e-01, -4.2213e-02,  9.7399e-02, -4.3836e-02,  1.0243e-01,\n",
            "         -1.0009e-01, -7.1944e-02,  7.4439e-02, -1.0038e-01, -1.0327e-01,\n",
            "          5.3161e-02,  6.2355e-02, -5.0352e-02,  8.1351e-02,  2.2432e-03,\n",
            "          4.6447e-02,  1.0247e-01,  3.8232e-02,  8.9837e-02, -9.3185e-02,\n",
            "         -1.0325e-01,  8.8519e-02, -5.1487e-02, -6.4115e-02, -1.5130e-02,\n",
            "          1.8804e-02,  4.0074e-02,  3.5409e-02,  1.1358e-02,  4.0965e-02,\n",
            "         -5.9482e-02, -1.2862e-02,  8.7251e-03,  9.1661e-03, -1.1262e-02,\n",
            "          2.0882e-02,  1.6403e-02,  6.1808e-02,  3.2485e-02,  1.0800e-02,\n",
            "         -5.0873e-02, -3.9295e-02, -4.5332e-02,  4.1433e-02],\n",
            "        [ 5.2487e-02,  1.0302e-01, -9.4899e-03, -5.7540e-02, -6.2065e-02,\n",
            "          5.5409e-02, -7.3627e-02,  2.1482e-02,  5.3548e-02, -7.0117e-02,\n",
            "         -1.0423e-01,  5.9352e-02, -6.7813e-02,  6.4901e-02, -8.6976e-02,\n",
            "         -3.8182e-02, -8.4110e-02, -9.8763e-02,  3.4984e-02, -2.7801e-02,\n",
            "         -1.3223e-02,  3.8846e-02, -2.4761e-03, -9.7178e-02, -7.4123e-03,\n",
            "          7.5921e-02, -8.5968e-02, -8.2125e-02,  1.9591e-02, -9.5969e-02,\n",
            "          9.2433e-02, -9.3333e-02,  1.0265e-01,  9.6986e-02, -7.9519e-02,\n",
            "         -5.3160e-02, -9.4352e-02, -5.4027e-02, -6.7571e-02, -9.8310e-02,\n",
            "          8.2958e-02, -3.0369e-02,  1.0776e-01,  2.6109e-02, -5.1419e-02,\n",
            "          7.5431e-03, -1.0662e-02, -1.1603e-02, -4.4774e-02, -8.8605e-02,\n",
            "         -7.0945e-02,  7.1507e-02, -4.9618e-03,  8.3952e-02,  5.1604e-02,\n",
            "         -8.4641e-02,  1.8028e-02,  5.3431e-02, -6.8960e-02, -1.9060e-02,\n",
            "          3.5826e-02, -3.0236e-02, -5.9391e-03,  8.2250e-02,  5.6108e-02,\n",
            "         -1.8509e-02,  5.2953e-02, -1.0838e-01, -3.6391e-02, -9.4325e-02,\n",
            "         -4.9503e-02,  3.1227e-03,  1.0477e-01,  7.8214e-02,  5.8993e-02,\n",
            "          9.8550e-02, -1.0238e-01,  8.9369e-03,  4.8475e-03, -4.3235e-04,\n",
            "          2.5944e-02,  9.4394e-02, -4.8513e-02, -4.3038e-02],\n",
            "        [ 3.0825e-02,  6.2409e-02,  6.0974e-02,  3.6670e-02,  9.1238e-02,\n",
            "         -2.6338e-02,  8.2921e-02,  4.0068e-02,  8.6235e-02, -6.0851e-02,\n",
            "          8.1926e-02, -1.8589e-02,  3.0650e-02, -2.5290e-02, -9.7945e-02,\n",
            "          4.6419e-02, -9.3440e-02, -3.4343e-02,  8.1332e-03,  4.1897e-02,\n",
            "         -4.0132e-02, -7.5920e-02, -8.5887e-02,  5.8399e-02, -6.0608e-02,\n",
            "          9.3923e-02, -2.6352e-02,  4.3955e-02, -1.6584e-02,  8.2295e-02,\n",
            "         -2.1314e-02, -9.6882e-02,  2.4418e-02,  1.9152e-02,  6.7152e-02,\n",
            "         -5.1149e-02,  8.2486e-02, -3.1765e-02,  1.8322e-02,  1.1868e-02,\n",
            "          2.1712e-02, -3.0377e-02,  9.4706e-02, -8.3867e-04, -4.1186e-02,\n",
            "          4.2159e-03, -6.8427e-02,  8.2436e-02,  7.6090e-02, -6.3310e-02,\n",
            "         -6.3325e-02,  8.5819e-02, -6.2294e-02, -2.5278e-02, -1.0561e-01,\n",
            "          4.1492e-02,  2.0509e-02,  6.9674e-03,  3.2608e-02,  4.1610e-02,\n",
            "         -4.2967e-02, -1.0464e-01, -2.8344e-02, -1.0742e-01, -7.2514e-02,\n",
            "          1.5445e-02,  5.6294e-02, -6.8488e-02, -6.2355e-02,  1.0181e-01,\n",
            "          4.8648e-02,  9.1330e-02, -7.2170e-02, -8.5273e-02, -4.5945e-04,\n",
            "          6.1599e-02,  1.5898e-03,  5.5886e-02, -4.0475e-02, -3.5475e-03,\n",
            "          2.5197e-02, -1.6915e-02,  4.9358e-02,  1.0827e-02],\n",
            "        [-9.7347e-02,  3.4828e-02,  9.9064e-02, -8.4951e-03,  9.0131e-02,\n",
            "         -5.8996e-02,  5.6027e-02,  4.7986e-02,  3.2951e-02,  7.0320e-02,\n",
            "         -9.7756e-02, -8.5354e-02, -5.6996e-02,  4.8346e-02, -4.5842e-02,\n",
            "         -1.9250e-02,  1.0865e-01,  7.1677e-03,  3.4506e-03, -1.0589e-01,\n",
            "         -1.0702e-02,  3.3991e-02,  7.2910e-02, -1.0520e-01,  1.2450e-02,\n",
            "         -3.1050e-02,  1.6925e-03, -1.9149e-02,  5.0827e-02, -4.1264e-03,\n",
            "         -5.9350e-02,  1.0384e-01, -6.9598e-02, -5.4403e-02,  8.3574e-03,\n",
            "         -2.3477e-02,  9.4389e-02, -2.8335e-02, -3.3704e-02, -6.1378e-03,\n",
            "          2.6858e-02, -9.6799e-02,  7.3090e-02,  2.1652e-02, -7.5839e-02,\n",
            "          8.2248e-02, -6.8438e-04, -9.6615e-02,  2.2657e-03, -5.5315e-02,\n",
            "          3.6338e-02, -2.2182e-02, -2.8801e-02, -7.9294e-02, -5.4749e-02,\n",
            "          5.1022e-02, -1.0859e-01,  1.0452e-01,  7.8722e-02, -2.7248e-02,\n",
            "          1.7391e-02,  1.0752e-01, -6.1425e-02, -4.8364e-02, -5.5888e-02,\n",
            "          8.8868e-02, -4.5883e-02,  7.6912e-02, -6.1779e-02,  8.5017e-03,\n",
            "          5.2116e-02, -5.5666e-02, -3.4105e-02,  4.2132e-02, -5.7031e-02,\n",
            "         -8.8212e-02, -8.9350e-02, -3.7039e-02, -1.0151e-01, -3.4223e-02,\n",
            "         -6.5036e-02,  3.8042e-02,  6.7237e-02, -4.9177e-02],\n",
            "        [-9.3452e-02, -7.7191e-02, -5.5627e-02, -9.1678e-02,  9.6932e-02,\n",
            "          7.0731e-03,  7.9203e-02, -8.0499e-02,  5.5115e-02,  1.5751e-02,\n",
            "         -6.4874e-02, -4.2241e-02,  1.6444e-02, -7.3635e-02, -9.3052e-02,\n",
            "         -5.2052e-02, -8.9936e-02,  3.1359e-02, -4.5374e-02,  8.1564e-02,\n",
            "         -7.1034e-02, -5.7757e-02, -1.9035e-02, -2.0427e-02, -2.5221e-02,\n",
            "          5.0697e-02,  8.2012e-02, -1.5977e-03,  2.3068e-02, -4.7000e-02,\n",
            "         -1.0906e-01,  1.0182e-01, -3.3646e-02, -2.3854e-02, -8.7005e-02,\n",
            "          7.9393e-02,  3.0552e-02,  8.8200e-02,  4.0751e-02,  1.4012e-02,\n",
            "         -7.5462e-02,  2.2494e-02,  2.9196e-02,  4.4903e-02,  1.0799e-01,\n",
            "          8.6288e-03, -2.0261e-02,  9.2253e-02,  4.4134e-02,  1.0356e-01,\n",
            "          1.1355e-02,  3.6818e-03,  9.8980e-02,  6.2554e-03, -1.5658e-02,\n",
            "         -3.2439e-02,  5.1701e-02, -3.3837e-02,  7.0204e-03, -5.6286e-02,\n",
            "         -7.1184e-02,  4.9832e-02, -2.8008e-02, -1.6175e-02, -1.0336e-01,\n",
            "          4.8702e-02, -2.0221e-03, -4.7061e-02,  9.7235e-02,  5.3388e-03,\n",
            "         -5.5813e-02, -1.5420e-03, -3.1542e-02, -4.1707e-02, -8.2693e-02,\n",
            "         -9.4787e-02,  5.9985e-02,  1.0518e-01, -2.1814e-03, -8.4722e-02,\n",
            "          2.2903e-02, -6.6022e-02,  1.0982e-02,  6.7499e-03],\n",
            "        [-1.1115e-02,  6.3300e-02,  8.7013e-02,  1.0365e-01,  1.9358e-03,\n",
            "         -1.4156e-02,  3.6088e-02,  3.8001e-02,  2.6078e-02,  1.0123e-01,\n",
            "          2.0313e-02,  2.8065e-03, -6.2678e-02,  1.0039e-02,  2.9842e-02,\n",
            "         -1.2590e-02, -4.8953e-02, -9.8177e-03, -4.9233e-02, -1.9408e-02,\n",
            "         -4.7774e-03,  8.9549e-02,  1.0457e-02,  3.9146e-02,  5.2301e-02,\n",
            "         -6.3051e-04,  5.6935e-02,  3.8543e-02, -4.5776e-02,  1.0267e-02,\n",
            "          1.0123e-02,  8.7663e-02,  9.3402e-02,  8.0215e-02, -1.6501e-02,\n",
            "         -7.1104e-02, -9.9038e-02,  2.8094e-02, -7.0248e-02,  3.9441e-02,\n",
            "         -3.2184e-02, -7.3027e-02,  4.8363e-02, -4.1713e-02,  4.6202e-02,\n",
            "          6.0016e-02,  3.0013e-03, -1.0730e-01,  1.7097e-02, -8.1422e-02,\n",
            "          7.0940e-02,  9.5075e-02,  9.2363e-02,  8.4007e-03,  1.0634e-01,\n",
            "         -1.0597e-01,  4.5429e-02,  8.4414e-02, -1.0697e-01,  8.3155e-02,\n",
            "          3.7531e-02,  1.0113e-01,  9.9451e-02,  2.2678e-02, -1.0145e-01,\n",
            "          3.5396e-02, -1.0488e-01,  2.0896e-02, -7.8262e-02,  2.5016e-02,\n",
            "          7.3192e-02, -5.2621e-02,  6.6523e-02, -9.3840e-02, -1.3478e-02,\n",
            "          1.0812e-01, -7.3118e-03, -5.0880e-02, -3.0459e-02, -6.6059e-02,\n",
            "         -4.2171e-02,  2.4520e-02, -4.4911e-02, -8.7021e-02],\n",
            "        [-3.4794e-02, -7.7001e-02, -6.8113e-02, -1.3155e-02, -7.5976e-02,\n",
            "         -3.8307e-02, -4.4908e-02,  6.6710e-02,  2.6707e-02, -3.7515e-02,\n",
            "          3.1935e-02,  7.4773e-02,  3.3477e-02,  8.0850e-04,  1.8615e-02,\n",
            "         -3.2473e-02, -1.3938e-02,  1.9698e-02, -7.6723e-02,  9.1602e-02,\n",
            "         -1.6756e-02,  3.8039e-02,  7.7992e-02,  7.9414e-02,  7.0487e-02,\n",
            "         -4.3360e-02, -6.7624e-03,  9.5167e-03,  6.5421e-02, -1.1167e-02,\n",
            "          9.5406e-02,  7.4556e-02,  4.0994e-02,  4.5604e-04,  7.5557e-02,\n",
            "          6.2758e-02,  4.8543e-02,  2.2582e-02,  7.5272e-02,  6.7874e-02,\n",
            "          8.8141e-02,  6.6061e-02, -9.9144e-02,  5.9170e-02, -7.7423e-02,\n",
            "         -2.2581e-02, -8.1304e-03, -8.4206e-02,  1.5095e-02,  5.3647e-02,\n",
            "          9.6442e-02,  4.2010e-02,  7.2335e-02,  4.4204e-03,  9.4194e-02,\n",
            "          3.5799e-02,  6.3976e-02, -6.5497e-02,  1.1973e-02,  4.4441e-02,\n",
            "          3.9483e-02, -1.1755e-02,  4.8461e-02, -1.0734e-01, -4.7172e-03,\n",
            "          1.0860e-02,  1.0456e-01,  5.3121e-02,  7.2797e-02, -6.0271e-02,\n",
            "          6.0316e-02, -4.6270e-02,  5.2793e-02,  6.4519e-02, -9.9277e-02,\n",
            "         -2.4558e-02,  5.3869e-02,  9.8981e-02,  1.6658e-02, -1.0113e-01,\n",
            "         -8.9723e-02,  5.9160e-02, -1.0033e-01,  5.9781e-02]],\n",
            "       requires_grad=True)), ('bias_orig', Parameter containing:\n",
            "tensor([ 0.0000, -0.0262,  0.0000, -0.0926,  0.0000,  0.0428,  0.0000, -0.0424,\n",
            "         0.0000,  0.0498], requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPK0kumiiq96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21da1300-10de-4323-e0a9-d42c388092bb"
      },
      "source": [
        "# Custom Mask Applied to the Bias values\n",
        "print(list(model.fc3.named_buffers()))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias_mask', tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG9qY9yXi6Rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bde4681-11cb-4ad9-a659-044134b7d864"
      },
      "source": [
        "# Access the 'bias_mask' from the Model\n",
        "print(model.fc3.bias_mask)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lle3BDllVJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee9010a4-9edd-40a1-f0fa-f4e4d791f0c9"
      },
      "source": [
        "model.fc3._forward_pre_hooks"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([(13, <__main__.customPruningMethod at 0x7f2ec98922b0>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k651QcoxKfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f427fedf-bed2-4698-b706-1f92c0aa2e52"
      },
      "source": [
        "# The above pruning  will zero out all the connections corresponding to\n",
        "# 50% (3 out of 6) of the channels, while preserving the action of the\n",
        "# previous mask.\n",
        "print(module.weight)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0000,  0.0000,  0.0407,  0.0687, -0.0731, -0.0717,  0.0000, -0.0000,\n",
            "          0.0559,  0.0000, -0.0000,  0.1006, -0.0579, -0.0000, -0.0556, -0.0644,\n",
            "          0.0444, -0.0422, -0.0488, -0.0573,  0.0707, -0.0927,  0.0407,  0.0000,\n",
            "          0.1050,  0.0000, -0.0963, -0.0000,  0.0000, -0.0000, -0.0000,  0.0911,\n",
            "         -0.0000,  0.0627, -0.0000, -0.0000, -0.0000, -0.1035,  0.0670,  0.0689,\n",
            "          0.0956,  0.0000,  0.0932,  0.0000, -0.0000, -0.0000,  0.0969,  0.0000,\n",
            "         -0.0000, -0.0926, -0.1000,  0.0536,  0.1029,  0.0457, -0.0000, -0.0838,\n",
            "         -0.0846, -0.0883, -0.0000, -0.0800, -0.0601, -0.0663,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000, -0.0981, -0.0000,  0.0756, -0.0685, -0.0000, -0.0656,\n",
            "          0.0446, -0.0808,  0.0861, -0.0000,  0.0736, -0.0000, -0.1009,  0.0430,\n",
            "          0.0000, -0.0000, -0.0419, -0.0808],\n",
            "        [ 0.1074,  0.0989, -0.0910, -0.1057, -0.0538,  0.0886, -0.0662,  0.0798,\n",
            "          0.0652,  0.0880, -0.0934, -0.0956,  0.0000, -0.0769,  0.0693,  0.0533,\n",
            "          0.0969,  0.0000,  0.0000, -0.0000, -0.0000,  0.0722,  0.0879,  0.0558,\n",
            "          0.0000,  0.0000,  0.0000, -0.0000,  0.0493,  0.0998,  0.1042,  0.0000,\n",
            "          0.0723, -0.0973, -0.0979, -0.0000,  0.0500,  0.0000, -0.0000,  0.0000,\n",
            "          0.0000,  0.0730,  0.0000,  0.0000, -0.0648,  0.0977, -0.0000,  0.0960,\n",
            "          0.0975,  0.0000, -0.0729, -0.0524,  0.0923, -0.0000,  0.1000,  0.0000,\n",
            "          0.0742,  0.0703, -0.0574, -0.0934,  0.0885, -0.0000, -0.0838, -0.0000,\n",
            "          0.0748,  0.0000,  0.0000,  0.0000, -0.0478, -0.0603, -0.0000, -0.0716,\n",
            "          0.0796,  0.0000,  0.0428,  0.0000,  0.1056, -0.0411,  0.0000,  0.0000,\n",
            "          0.0000, -0.1061,  0.0000, -0.0712],\n",
            "        [ 0.1059, -0.0447, -0.0642, -0.0466, -0.0688,  0.0805,  0.0000, -0.0438,\n",
            "         -0.0000, -0.0543,  0.0757,  0.0657, -0.1012,  0.0000,  0.0464,  0.0671,\n",
            "         -0.0894, -0.0000,  0.0908, -0.0729, -0.0000, -0.0930, -0.0921,  0.0000,\n",
            "          0.0000,  0.0000, -0.0807, -0.1088,  0.0000,  0.0000, -0.0971, -0.0756,\n",
            "          0.0000, -0.0525,  0.0501,  0.0000, -0.0000,  0.0000,  0.0881,  0.0000,\n",
            "         -0.0000, -0.0000,  0.1084, -0.0543, -0.0826, -0.0000, -0.0982, -0.1081,\n",
            "         -0.0442,  0.0632,  0.0967, -0.0468, -0.0000,  0.0000,  0.0981, -0.0000,\n",
            "          0.0000,  0.0443, -0.0690, -0.0869,  0.0000,  0.0481,  0.0000, -0.0000,\n",
            "         -0.0959, -0.0422,  0.0837,  0.0000,  0.0855, -0.0000,  0.1059,  0.0000,\n",
            "          0.0000,  0.1043, -0.0000, -0.0000, -0.0000,  0.0000,  0.0456, -0.0514,\n",
            "         -0.0820, -0.0406,  0.0000,  0.0488],\n",
            "        [-0.0000,  0.0735, -0.0000,  0.0000, -0.0000,  0.0000,  0.0851, -0.0000,\n",
            "          0.0000, -0.1089, -0.0532,  0.0554,  0.0987,  0.0786,  0.0583, -0.0444,\n",
            "          0.0747,  0.0595,  0.0709,  0.0000, -0.0000,  0.0000, -0.0000, -0.0824,\n",
            "          0.0875, -0.0538,  0.0528, -0.1035,  0.0740, -0.0503,  0.0874, -0.0000,\n",
            "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0950,  0.0000,  0.0000, -0.1064,\n",
            "          0.0806,  0.0000,  0.0792,  0.0000, -0.0884, -0.0000,  0.1010,  0.0000,\n",
            "         -0.0000,  0.0675, -0.0524,  0.0878, -0.1068, -0.0713,  0.0604,  0.0000,\n",
            "         -0.0838,  0.0555, -0.0448, -0.0409, -0.0000, -0.0478, -0.0000,  0.0853,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.1080,  0.0963,  0.0000,\n",
            "         -0.1000, -0.0603, -0.0000,  0.0526, -0.1075,  0.0482, -0.1089, -0.0772,\n",
            "          0.0510, -0.0508,  0.0000,  0.0000],\n",
            "        [ 0.0776,  0.1064,  0.0000,  0.0000, -0.0476, -0.0000, -0.0000, -0.0916,\n",
            "         -0.0807,  0.0000,  0.0000, -0.0699,  0.0975,  0.1026, -0.0999,  0.0000,\n",
            "          0.1001, -0.0000,  0.0000, -0.1017,  0.0000, -0.1057, -0.0000,  0.0992,\n",
            "         -0.0000, -0.0536,  0.0000, -0.0000,  0.0000, -0.0000, -0.0584,  0.0447,\n",
            "          0.0000, -0.0909, -0.0000,  0.0619, -0.0531,  0.0558,  0.0000, -0.1023,\n",
            "         -0.0847,  0.0000,  0.0000,  0.0000,  0.0992, -0.0989,  0.0660,  0.0000,\n",
            "         -0.0629, -0.0000,  0.0000,  0.0000, -0.0708,  0.0925,  0.0606,  0.0000,\n",
            "         -0.0000,  0.0751, -0.0426, -0.0974,  0.0515,  0.0899,  0.0909, -0.0000,\n",
            "          0.0796, -0.0463,  0.0491,  0.0000,  0.0413, -0.0436, -0.0000, -0.0952,\n",
            "         -0.0000,  0.0650,  0.0000,  0.0592,  0.0000,  0.0686,  0.0493,  0.0724,\n",
            "          0.0849,  0.0000, -0.0000,  0.1067],\n",
            "        [ 0.0000, -0.0661, -0.0000,  0.0518, -0.0986, -0.0000,  0.0546, -0.0000,\n",
            "         -0.0915,  0.1040, -0.0855,  0.0912, -0.0515,  0.0791,  0.0823, -0.0924,\n",
            "          0.0000, -0.0914,  0.0000, -0.0961, -0.1013, -0.0702, -0.0537, -0.1038,\n",
            "         -0.1067,  0.0728, -0.0000,  0.0554,  0.0775, -0.1085,  0.0000,  0.0739,\n",
            "          0.0000,  0.0000,  0.0942,  0.0767,  0.1026,  0.0000,  0.0645, -0.1034,\n",
            "          0.0000,  0.0866,  0.1091,  0.0000, -0.0653, -0.1057, -0.0000, -0.0000,\n",
            "          0.0673,  0.0783, -0.0000,  0.0478,  0.0000,  0.0678, -0.1084,  0.0000,\n",
            "         -0.0430, -0.0000,  0.0000, -0.0000, -0.0000, -0.0850, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000, -0.0000,  0.0972,  0.0942,  0.0662, -0.0529,  0.0000,\n",
            "          0.0000, -0.0766, -0.0955,  0.0000, -0.0000, -0.1065,  0.0765,  0.0000,\n",
            "         -0.0000, -0.1005, -0.0700,  0.0000],\n",
            "        [ 0.0000,  0.0853,  0.0000, -0.0000, -0.0000, -0.0655,  0.0833, -0.0674,\n",
            "          0.0000, -0.1045,  0.0545,  0.0801, -0.0000,  0.0811,  0.0408,  0.0000,\n",
            "         -0.0000,  0.0815,  0.0000, -0.0000,  0.0559,  0.0617, -0.0000, -0.0000,\n",
            "         -0.0000,  0.1008, -0.0619,  0.0000,  0.0763,  0.0000, -0.1040,  0.0682,\n",
            "         -0.1037,  0.0880,  0.0865,  0.0000, -0.0822, -0.0000, -0.0000, -0.0519,\n",
            "         -0.0000,  0.1022,  0.0522,  0.0915, -0.0456, -0.0000, -0.0732, -0.0498,\n",
            "         -0.0652, -0.0826,  0.0000,  0.0000, -0.0405,  0.0971,  0.0653,  0.0805,\n",
            "          0.0547,  0.1047, -0.0574, -0.0808, -0.0000, -0.0758,  0.0739,  0.0861,\n",
            "          0.0638,  0.0431, -0.0626, -0.0553,  0.0656, -0.0000, -0.0500, -0.0000,\n",
            "          0.0000, -0.0000,  0.0000,  0.0878, -0.0000, -0.0000, -0.0918,  0.0782,\n",
            "          0.0778, -0.0000, -0.0473, -0.0000],\n",
            "        [-0.0806,  0.0000,  0.0756,  0.0000, -0.0711,  0.0609, -0.0743, -0.0000,\n",
            "         -0.0405,  0.0854, -0.0000, -0.0000,  0.0000, -0.0661, -0.0800,  0.0703,\n",
            "          0.0547, -0.0000, -0.0685,  0.0897, -0.0816,  0.0785, -0.0855,  0.0741,\n",
            "          0.0000,  0.0791,  0.0000,  0.0000,  0.0000, -0.0000, -0.0779,  0.0502,\n",
            "         -0.0957, -0.0000,  0.0000, -0.0696, -0.0000,  0.0421, -0.0526, -0.0666,\n",
            "         -0.0928, -0.0491, -0.1027, -0.0000,  0.0968,  0.0000,  0.0610,  0.0000,\n",
            "          0.0636, -0.0000,  0.0000, -0.1054, -0.0000, -0.0000, -0.0000,  0.0801,\n",
            "          0.0912,  0.0649, -0.0992, -0.0000, -0.0000, -0.0666, -0.0576,  0.0455,\n",
            "          0.0511, -0.0000, -0.1000,  0.0770,  0.0851,  0.0837, -0.0864,  0.0648,\n",
            "          0.0474, -0.0000,  0.0000, -0.0577,  0.0000,  0.0000,  0.0478,  0.0635,\n",
            "          0.0977, -0.0666, -0.1025,  0.0000],\n",
            "        [-0.0895, -0.0000,  0.0694, -0.0000, -0.0000, -0.0557,  0.1021, -0.0787,\n",
            "          0.0905,  0.1062, -0.0000,  0.0000,  0.0837,  0.0474,  0.0870,  0.0991,\n",
            "         -0.0504,  0.0816,  0.1050,  0.0988,  0.0824,  0.0000, -0.0000, -0.0920,\n",
            "          0.0000,  0.0464, -0.0966, -0.0589, -0.0000,  0.0673,  0.0491, -0.0793,\n",
            "          0.0000,  0.0889,  0.0565,  0.1058, -0.0419, -0.1049, -0.0000,  0.0503,\n",
            "          0.0000, -0.0748,  0.0901, -0.0606, -0.0675, -0.0000,  0.0000,  0.1078,\n",
            "          0.0000,  0.0000, -0.0417,  0.0000,  0.0000, -0.1047, -0.0798, -0.0480,\n",
            "          0.0935,  0.0000, -0.1038, -0.0000, -0.0933, -0.0764, -0.0427, -0.0413,\n",
            "          0.0982,  0.0000,  0.0755, -0.0530,  0.0867,  0.0717,  0.0506, -0.0974,\n",
            "         -0.0931,  0.0000, -0.0000, -0.0000,  0.1051,  0.0000, -0.0000, -0.0000,\n",
            "          0.1031,  0.0621, -0.0000, -0.1062],\n",
            "        [-0.0000, -0.0995,  0.0698,  0.0715, -0.0666, -0.0000,  0.0721, -0.0834,\n",
            "          0.0502,  0.0000, -0.0874, -0.0739,  0.0597,  0.0000,  0.0768,  0.0746,\n",
            "          0.0460, -0.0785, -0.0576,  0.0000,  0.0729,  0.0000, -0.0000,  0.0463,\n",
            "          0.0000,  0.0000, -0.0477,  0.0000, -0.1005,  0.0000, -0.0000, -0.0000,\n",
            "          0.0493, -0.0000,  0.0773,  0.0572, -0.0923,  0.0000,  0.0000, -0.0675,\n",
            "          0.0516,  0.0000,  0.1039,  0.0000,  0.1036, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0472, -0.0000,  0.0716,  0.0752,  0.0000, -0.0000, -0.0500,\n",
            "          0.0602,  0.0597, -0.0931,  0.0000,  0.0000,  0.0656, -0.0000, -0.0000,\n",
            "         -0.0540, -0.0860, -0.0000,  0.0836, -0.0000, -0.0000, -0.0000,  0.0544,\n",
            "         -0.0937,  0.0736,  0.0000,  0.0000,  0.0900,  0.0000,  0.0709,  0.0416,\n",
            "          0.0000, -0.0000,  0.0548, -0.0773]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlMD1J5Iyyyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d745a34a-a816-469b-da0f-735e1a64522c"
      },
      "source": [
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([(7, <torch.nn.utils.prune.L1Unstructured object at 0x7f2ec98eb6a0>)])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}