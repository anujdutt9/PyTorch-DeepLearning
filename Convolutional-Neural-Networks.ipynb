{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook shows how to write a CNN in PyTorch for MNIST dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1120bce30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Seed for reporoducability\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# root: directory for downloading the data or to look for data\n",
    "# train: True during training\n",
    "# transform: Converts a PIL.Image or numpy.ndarray to torch.FloatTensor of shape (C x H x W) \n",
    "#            and normalize it in the range [0.0, 1.0].\n",
    "# download: True, if you want to download the data\n",
    "training_Data = MNIST(\n",
    "                root = \"./dataset/\",\n",
    "                train = True,\n",
    "                transform = transforms.ToTensor(),\n",
    "                download = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Images:  torch.Size([60000, 28, 28])\n",
      "Size of Training Labels:  torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training Images: \", training_Data.train_data.size())\n",
    "print(\"Size of Training Labels: \", training_Data.train_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True Label: tensor(5)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Sample data\n",
    "img = training_Data.train_data[0].numpy()\n",
    "label = training_Data.train_labels[0]\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(\"True Label: \"+ str(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "\n",
    "# Epochs\n",
    "epochs = 2\n",
    "\n",
    "# Batch size\n",
    "batchSize = 50\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for Batching the Training Data\n",
    "train_data_loader = Data.DataLoader(\n",
    "              dataset = training_Data,\n",
    "              batch_size = batchSize,\n",
    "              shuffle = True,\n",
    "              num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 1, 28, 28]), torch.Size([2000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test Data\n",
    "# Convert Test Data into Variable\n",
    "test_data = MNIST(root='./dataset/', train=False)\n",
    "\n",
    "# Test Features\n",
    "X_test = Variable(torch.unsqueeze(test_data.test_data, dim=1)).type(torch.FloatTensor)[:2000]/255.\n",
    "\n",
    "# Test Labels\n",
    "y_test = test_data.test_labels[:2000]\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Function\n",
    "class CNNModel(nn.Module):\n",
    "    \n",
    "    # Initialize Values\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Define 1st Convolutional Layer\n",
    "        \n",
    "        # Input Shape: (1,28,28)\n",
    "        # in_channel: Number of Inputs\n",
    "        # out_channel: number of filters\n",
    "        # kernel_stride: kernel/filter size\n",
    "        # stride: filter movement/step\n",
    "        # padding: pad image with 0's for equal width and height after this layer or not\n",
    "        self.conv_1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2), # Output Shape: [16, 28, 28]\n",
    "                                   nn.ReLU(), \n",
    "                                   nn.MaxPool2d(kernel_size=2))  # Output Shape: [16, 14, 14]\n",
    "        \n",
    "        # Define 2nd Convolutional Layer\n",
    "        \n",
    "        # Input Shape: [16, 14, 14]\n",
    "        # in_channel: Number of Inputs\n",
    "        # out_channel: number of filters\n",
    "        # kernel_stride: kernel/filter size\n",
    "        # stride: filter movement/step\n",
    "        # padding: pad image with 0's for equal width and height after this layer or not\n",
    "        self.conv_2 = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2), # Output Shape: [32,14, 14]\n",
    "                                   nn.ReLU(), \n",
    "                                   nn.MaxPool2d(kernel_size=2)) # Output Shape: [32,7,7]\n",
    "        \n",
    "        # Output Layer\n",
    "        # 10 classes\n",
    "        self.out = nn.Linear(32*7*7, 10)\n",
    "        \n",
    "    # Forward Pass\n",
    "    def forward(self, x):\n",
    "        # Conv Layer 1\n",
    "        x = self.conv_1(x)\n",
    "        # Conv Layer 2\n",
    "        x = self.conv_2(x)\n",
    "        # Flatten the Output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Output Layer\n",
    "        output = self.out(x)\n",
    "        return output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (conv_1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "cnn = CNNModel()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CNNModel(\n",
       "  (conv_1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Criteria\n",
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data with Labels\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.figure(figsize = (20,6))\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer');\n",
    "plt.pause(0.01)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
